{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from dbnn.dbnn import DBNN\n",
    "from dbnn.och import OCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig = False\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "\n",
    "if not savefig:\n",
    "    plt.rcParams[\"figure.figsize\"] = (4, 4)\n",
    "    plt.rcParams[\"font.size\"] = 15\n",
    "    plt.rcParams[\"figure.titlesize\"] = 25\n",
    "    plt.rcParams[\"axes.labelsize\"] = 20\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "    plt.rcParams[\"legend.fontsize\"] = 13\n",
    "    plt.rcParams[\"lines.linewidth\"] = 2\n",
    "else:\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    plt.rcParams[\"font.size\"] = 30\n",
    "    plt.rcParams[\"axes.labelsize\"] = 53\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "    plt.rcParams[\"legend.fontsize\"] = 28\n",
    "    plt.rcParams[\"lines.linewidth\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants and hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "dataset_root = 'datasets/'\n",
    "dnn_path = 'models_checkpoints/dnn'\n",
    "bnn_path = 'models_checkpoints/bnn'\n",
    "\n",
    "# experiments\n",
    "seed = 30\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# och parameters\n",
    "och_x1_params = {'k': 5, 'l': 5.0, 's': 1.0}\n",
    "och_x_params = {'k': 5, 'l': 1.0, 's': 1.0}\n",
    "och_y_params = {'k': 5, 'l': 1.0, 's': 1.0}\n",
    "\n",
    "# style\n",
    "alpha = 0.12\n",
    "colors = [\"tab:blue\", \"tab:green\", \"tab:purple\", \"tab:red\"]\n",
    "labels = [\"DNN\", \"MU\", \"DU\", \"DBNN\"]\n",
    "guide_linestyle=(0, (1, 1))\n",
    "linestyles = [(0, (5, 1)), 'solid', (0, (5, 1)), 'solid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_normal(model, xs, regularizer=None): \n",
    "    logits = [tf.expand_dims(logit, axis=-1) for logit in tf.unstack(model(xs), axis=-1)]\n",
    "    if regularizer is not None:\n",
    "        logits[1] = tf.math.maximum(logits[1], math.log(math.exp(regularizer) - 1.))\n",
    "    return tfd.Normal(loc=logits[0], scale=tf.math.softplus(logits[1]))\n",
    "\n",
    "def bnn_categorical(model, xs):\n",
    "    logits = model(xs)\n",
    "    return tfd.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, x_train, y_train, batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    indexes_batch = np.array_split(np.random.permutation(len(x_train)), len(x_train) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "\n",
    "    for indexes in indexes_batch:\n",
    "        xs = tf.stack([x_train[index] for index in indexes])\n",
    "        ys = tf.stack([y_train[index] for index in indexes])\n",
    "        train_step(model, xs, ys, optimizer, loss_ftn, *loss_metrics)\n",
    "    \n",
    "def train_step(model, x_batch, y_batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    with tf.GradientTape() as tape:\n",
    "        losses = loss_ftn(model, x_batch, y_batch)\n",
    "    gradients = tape.gradient(losses[0], model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    for loss, loss_metric in zip(losses, loss_metrics):\n",
    "        loss_metric(loss)\n",
    "        \n",
    "def dnn_loss_ftn(model, xs, ys, loss_obj):\n",
    "    return loss_obj(ys, model(xs)),\n",
    "\n",
    "def bnn_normal_loss_ftn(model, xs, ys, length, regularizer=None):\n",
    "    nll = - tf.reduce_mean(bnn_normal(model, xs, regularizer).log_prob(ys))\n",
    "    kl = sum(model.losses) / length\n",
    "    loss = nll + kl\n",
    "    return loss, nll\n",
    "\n",
    "def bnn_categorical_loss_ftn(model, xs, ys, length):\n",
    "    ys = tf.squeeze(ys, axis=-1)\n",
    "    nll = - tf.reduce_mean(bnn_cateborical(xs).log_prob(ys))\n",
    "    kl = sum(model.losses) / length\n",
    "    loss = nll + kl\n",
    "    return loss, nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "* [Toy Example of Regression](#Toy-Example-of-Regression)\n",
    "* [Occupancy Detection Data Set (Classification)](#Occupancy-Detection-Data-Set-(Classification))\n",
    "* [Air Quality Data Set (Regression)](#Air-Quality-Data-Set-(Regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example of Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOY_REGRESSION_FLAG = \"toy-regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def create_bnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(10, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num = 50\n",
    "x_train = np.linspace(-3, 3, num=num)\n",
    "y_train = np.cos(x_train) + np.random.normal(0, 0.1, size=num)\n",
    "x_train = x_train.astype(np.float32).reshape((num, 1))\n",
    "y_train = y_train.astype(np.float32).reshape((num, 1))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.set_xlim(-5.2, 5.2)\n",
    "ax.set_ylim(-3.2, 3.2)\n",
    "ax.scatter(x_train, y_train, color=\"black\", s=30, marker=\"+\", label=\"Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 150, 10\n",
    "\n",
    "dnn = create_dnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_obj = tf.keras.losses.MSE\n",
    "loss_ftn = lambda model, xs, ys: dnn_loss_ftn(model, xs, ys, loss_obj)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(dnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        template = 'Epoch {}, Loss: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        \n",
    "dnn.save_weights(\"%s-%s\" % (dnn_path, TOY_REGRESSION_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 300, 10\n",
    "\n",
    "bnn = create_bnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_ftn = lambda model, xs, ys: bnn_normal_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "        \n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        template = 'Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (bnn_path, TOY_REGRESSION_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-5, 5, num=50)\n",
    "x_pred = x_pred.astype(np.float32).reshape((50, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = create_dnn()\n",
    "dnn.load_weights(\"%s-%s\" % (dnn_path, TOY_REGRESSION_FLAG))\n",
    "\n",
    "dnn_x_pred, dnn_y_pred, dnn_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    dnn_x_pred.append(x)\n",
    "    dnn_y_pred.append(dnn(tf.stack([x]))[0].numpy())\n",
    "    time2 = time.time()\n",
    "    dnn_runtimes.append(time2 - time1)\n",
    "    \n",
    "print(\"DNN Runtime: %.3f ± %.3f (ms)\" % (np.mean(dnn_runtimes) * 10 ** 3, np.std(dnn_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no = 5\n",
    "bnn = create_bnn()\n",
    "bnn.load_weights(\"%s-%s\" % (bnn_path, TOY_REGRESSION_FLAG))\n",
    "\n",
    "mu_x_pred, mu_y_pred, mu_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    for _ in range(sample_no):\n",
    "        mu_x_pred.append(x)\n",
    "        mu_y_pred.append(bnn_normal(bnn, tf.stack([x])).sample()[0].numpy())\n",
    "    time2 = time.time()\n",
    "    mu_runtimes.append(time2 - time1)\n",
    "    \n",
    "print(\"MU Runtime: %.3f ± %.3f (ms)\" % (np.mean(mu_runtimes) * 10 ** 3, np.std(mu_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_no = 5\n",
    "\n",
    "x_dims, y_dims = [1], [1]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "du = DBNN(lambda x: dnn(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "du_x_pred, du_y_pred, du_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    du.update(x)\n",
    "    time2 = time.time()\n",
    "    du_runtimes.append(time2 - time1)\n",
    "    for _ in range(sample_no):\n",
    "        _x_pred, _y_pred = du.och_x.sample(), du.och_y.sample()\n",
    "        if _x_pred is not None and _y_pred is not None:\n",
    "            du_x_pred.append(_x_pred[0])\n",
    "            du_y_pred.append(_y_pred.numpy())\n",
    "\n",
    "print(\"DU Runtime: %.3f ± %.3f (ms)\" % (np.mean(du_runtimes) * 10 ** 3, np.std(du_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_no = 5\n",
    "\n",
    "x_dims, y_dims = [1], [1]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "dbnn = DBNN(lambda x: bnn_normal(bnn, tf.stack([x[0]])).sample(), och_x_1, och_x, och_y)\n",
    "\n",
    "dbnn_x_pred, dbnn_y_pred, dbnn_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    dbnn.update(x)\n",
    "    time2 = time.time()\n",
    "    dbnn_runtimes.append(time2 - time1)\n",
    "    for _ in range(sample_no):\n",
    "        _x_pred, _y_pred = dbnn.och_x.sample(), dbnn.och_y.sample()\n",
    "        if _x_pred is not None and _y_pred is not None:\n",
    "            dbnn_x_pred.append(_x_pred[0])\n",
    "            dbnn_y_pred.append(_y_pred.numpy())\n",
    "\n",
    "print(\"DBNN Runtime: %.3f ± %.3f (ms)\" % (np.mean(dbnn_runtimes) * 10 ** 3, np.std(dbnn_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18, 4))\n",
    "datasets = [(dnn_x_pred, dnn_y_pred), (mu_x_pred, mu_y_pred), (du_x_pred, du_y_pred), (dbnn_x_pred, dbnn_y_pred)]\n",
    "for ax, preds, label, color in zip(axes, datasets, [\"DNN\", \"MU\", \"DU\", \"DBNN\"], colors):\n",
    "    ax.set_xlim(-5.2, 5.2)\n",
    "    ax.set_ylim(-3.2, 3.2)\n",
    "    ax.scatter(x_train, y_train, color=\"black\", s=30, marker=\"+\", label=\"Training\")\n",
    "    ax.scatter(preds[0], preds[1], facecolors=\"none\", edgecolors=color, label=label)\n",
    "    ax.legend(edgecolor='black', fancybox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Occupancy Detection Data Set](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCC_FLAG = \"occupancy\"\n",
    "OCC_INPUT_DIM = 5\n",
    "OCC_CLASS_NO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_file_names, test_file_names = [\"datatraining.txt\"], [\"datatest.txt\", \"datatest2.txt\"]\n",
    "\n",
    "def occupancy_data(file_names):\n",
    "    occupancy_path = dataset_root + \"%s/\" % OCC_FLAG\n",
    "    x, y = [], []\n",
    "    for file_name in file_names:\n",
    "        with open(occupancy_path + file_name, 'r') as csvfile:\n",
    "            file_reader = csv.reader(csvfile, delimiter=',')\n",
    "            header = next(file_reader)\n",
    "            for row in file_reader:\n",
    "                x.append(tf.constant([float(v) for v in row[2: 7]]))\n",
    "                y.append(tf.constant([int(row[7])]))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(OCC_CLASS_NO, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "def create_bnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(50, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(50, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(OCC_CLASS_NO)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = occupancy_data(train_file_names)\n",
    "\n",
    "print(\"train dataset: %d, test datasets: %d\" % (len(x_train), len(x_test)))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 10, 10\n",
    "\n",
    "dnn = create_dnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_ftn = lambda model, xs, ys: dnn_loss_ftn(model, xs, ys, loss_obj)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(dnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = 'Epoch {}, Loss: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        \n",
    "dnn.save_weights(\"%s-%s\" % (dnn_path, OCC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs, batch = 20, 10\n",
    "\n",
    "bnn = create_bnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = 'Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (bnn_path, OCC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = occupancy_data(test_file_names)\n",
    "\n",
    "print(\"Testset size: \", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, xs, ys, runtime_metric, *loss_metrics):\n",
    "    for loss_ftn, metric in loss_metrics:\n",
    "        time1 = time.time()\n",
    "        for loss_ftn, metric in loss_metrics:\n",
    "            loss = loss_ftn(model, xs, ys)\n",
    "            if loss is not None:\n",
    "                metric(loss)\n",
    "        time2 = time.time()\n",
    "        runtime_metric(time2 - time1)\n",
    "        \n",
    "def test(model, x_test, y_test, batch, runtime_metric, *loss_metrics):\n",
    "    indexes_batch = np.array_split(range(len(x_test)), len(x_test) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "    for indexes in indexes_batch:\n",
    "        xs = tf.stack([x_test[index] for index in indexes])\n",
    "        ys = tf.stack([y_test[index] for index in indexes])\n",
    "        test_step(model, xs, ys, runtime_metric, *loss_metrics)\n",
    "        \n",
    "def msec_from_samples(ys, classes_no, sws):\n",
    "    return sum([tf.keras.losses.MSE(tf.one_hot(ys, classes_no), s) * w for s, w in sws])\n",
    "\n",
    "def mse_from_samples(ys, sws):\n",
    "    return sum([tf.keras.losses.MSE(ys, s) * w for s, w in sws])\n",
    "\n",
    "cce_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def cce_from_samples(ys, sws):\n",
    "    return sum([cce_object(ys, s) * w for s, w, in sws])\n",
    "\n",
    "def nll_from_samples(ys, sws, regularizer=None):\n",
    "    if regularizer is None:\n",
    "        regularizer = 0\n",
    "    \n",
    "    mean, variance = tf.nn.weighted_moments(\n",
    "        tf.stack([s for s, _ in sws]),\n",
    "        [0],\n",
    "        tf.stack([w for _, w in sws]))\n",
    "    if len(sws) > 1 and tf.math.reduce_mean(variance) > regularizer:\n",
    "        nll = - tf.reduce_mean(tfd.Normal(mean, tf.math.sqrt(variance)).log_prob(ys))\n",
    "    else:\n",
    "        nll = None\n",
    "    \n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = create_dnn()\n",
    "dnn.load_weights(\"%s-%s\" % (dnn_path, OCC_FLAG))\n",
    "\n",
    "bnn = create_bnn()\n",
    "bnn.load_weights(\"%s-%s\" % (bnn_path, OCC_FLAG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = 1\n",
    "\n",
    "sampling = lambda model, xs: [(model(xs), 1.0)]\n",
    "msec_ftn = lambda model, xs, ys: msec_from_samples(ys, OCC_CLASS_NO, sampling(model, xs))\n",
    "cce_ftn = lambda model, xs, ys: cce_from_samples(ys, sampling(model, xs))\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "test(dnn, x_test, y_test, batch, runtime_metric, (msec_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "        \n",
    "template = 'DNN Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch, sample_no = 1, 5\n",
    "\n",
    "sampling = lambda model, xs: [(tf.one_hot(bnn_categorical(model, xs).sample(), OCC_CLASS_NO), 1 / sample_no) for _ in range(sample_no)]\n",
    "msec_ftn = lambda model, xs, ys: msec_from_samples(ys, OCC_CLASS_NO, sampling(model, xs))\n",
    "cce_ftn = lambda model, xs, ys: cce_from_samples(ys, sampling(model, xs))\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "test(bnn, x_test, y_test, batch, runtime_metric, (msec_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "\n",
    "template = 'MU Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = 1\n",
    "\n",
    "x_dims, y_dims = [OCC_INPUT_DIM], [OCC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "du = DBNN(lambda x: dnn(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "def sampling(model, xs):\n",
    "    model.update(xs)\n",
    "    return model.och_y.cws()\n",
    "\n",
    "msec_ftn = lambda model, xs, ys: msec_from_samples(ys, OCC_CLASS_NO, sampling(model, xs))\n",
    "cce_ftn = lambda model, xs, ys: cce_from_samples(ys, sampling(model, xs))\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "\n",
    "test(du, x_test, y_test, batch, runtime_metric, (msec_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "\n",
    "template = 'DU Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DBNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1\n",
    "\n",
    "x_dims, y_dims = [OCC_CLASS_NO], [OCC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "dbnn = DBNN(lambda x: bnn_categorical(bnn, x[0]).sample(), och_x_1, och_x, och_y)\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "def sampling(model, xs):\n",
    "    model.update(xs)\n",
    "    return model.och_y.cws()\n",
    "\n",
    "msec_ftn = lambda model, xs, ys: msec_from_samples(ys, OCC_CLASS_NO, sampling(model, xs))\n",
    "cce_ftn = lambda model, xs, ys: cce_from_samples(ys, sampling(model, xs))\n",
    "\n",
    "test(du, x_test, y_test, batch, runtime_metric, (msec_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "\n",
    "template = 'DBNN Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Air Quality Data Set](https://archive.ics.uci.edu/ml/datasets/Air+quality) (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIR_FLAG = \"air\"\n",
    "AIR_INPUT_DIM = 12\n",
    "AIR_CLASS_NO = 1\n",
    "AIR_REGULARIZER = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8644 rows are ignored.\n",
      "train dataset: 744, test datasets: 83\n",
      "x sample:  [1.360e+03 1.500e+02 1.190e+01 1.046e+03 1.660e+02 1.056e+03 1.130e+02\n",
      " 1.692e+03 1.268e+03 1.360e+01 4.890e+01 7.578e-01]\n",
      "y sample:  [2.6]\n"
     ]
    }
   ],
   "source": [
    "def air_data():\n",
    "    air_path = dataset_root + \"%s/\" % AIR_FLAG\n",
    "    x, y = [], []\n",
    "    count = 0\n",
    "    file_name = \"AirQualityUCI.csv\"\n",
    "    with open(air_path + file_name, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=';')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            try:\n",
    "                if \"-200\" in row:\n",
    "                    raise ValueError()\n",
    "                x.append(tf.constant([float(v.replace(',', '.')) for v in row[3: 15]]))\n",
    "                y.append(tf.constant([float(row[2].replace(',', '.'))]))\n",
    "            except:\n",
    "                count += 1 \n",
    "    print(\"%d rows are ignored.\" % count)\n",
    "    return x, y\n",
    "\n",
    "xs, ys = air_data()\n",
    "x_train, y_train = xs[:int(len(xs) * 0.9)], ys[:int(len(ys) * 0.9)]\n",
    "x_test, y_test = xs[int(len(xs) * 0.9):], ys[int(len(ys) * 0.9):]\n",
    "\n",
    "print(\"train dataset: %d, test datasets: %d\" % (len(x_train), len(x_test)))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(AIR_CLASS_NO)\n",
    "    ])\n",
    "\n",
    "def create_bnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(50, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(50, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(AIR_CLASS_NO * 2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch = 200, 10\n",
    "\n",
    "dnn = create_dnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_obj = tf.keras.losses.MSE\n",
    "loss_ftn = lambda model, xs, ys: dnn_loss_ftn(model, xs, ys, loss_obj)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(dnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        template = 'Epoch {}, Loss: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        \n",
    "dnn.save_weights(\"%s-%s\" % (dnn_path, AIR_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch = 100, 10\n",
    "\n",
    "bnn = create_bnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_normal_loss_ftn(model, xs, ys, len(x_train), regularizer=AIR_REGULARIZER)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        template = 'Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (bnn_path, AIR_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2c0d53ce2b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn = create_dnn()\n",
    "dnn.load_weights(\"%s-%s\" % (dnn_path, AIR_FLAG))\n",
    "\n",
    "bnn = create_bnn()\n",
    "bnn.load_weights(\"%s-%s\" % (bnn_path, AIR_FLAG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Runtime: 7.243193626403809 (ms), MSE: 0.3162651062011719\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "\n",
    "sampling = lambda model, xs: [(model(xs), 1.0)]\n",
    "mse_ftn = lambda model, xs, ys: mse_from_samples(ys, sampling(model, xs))\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "\n",
    "test(dnn, x_test, y_test, batch, runtime_metric, (mse_ftn, mse_metric))\n",
    "        \n",
    "template = 'DNN Runtime: {} (ms), MSE: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MU Runtime: 255.9471435546875 (ms), MSE: 92826.71875, NLL: 6.56217622756958\n"
     ]
    }
   ],
   "source": [
    "batch, sample_no = 1, 5\n",
    "\n",
    "sampling = lambda model, xs: [(bnn_normal(model, xs, AIR_REGULARIZER).sample(), 1 / sample_no) for _ in range(sample_no)]\n",
    "mse_ftn = lambda model, xs, ys: mse_from_samples(ys, sampling(model, xs))\n",
    "nll_ftn = lambda model, xs, ys: nll_from_samples(ys, sampling(model, xs))\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "nll_metric = tf.keras.metrics.Mean(name='nll')\n",
    "\n",
    "test(bnn, x_test, y_test, batch, runtime_metric, (mse_ftn, mse_metric), (nll_ftn, nll_metric))\n",
    "\n",
    "template = 'MU Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      nll_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "nll_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-103f01e2d4c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DU Runtime: {} (ms), MSE: {}, NLL: {}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m print(template.format(runtime_metric.result() * 1000, \n\u001b[1;32m---> 23\u001b[1;33m                       mse_metric.result()))\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mruntime_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmse_metric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "\n",
    "x_dims, y_dims = [AIR_INPUT_DIM], [AIR_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "du = DBNN(lambda x: dnn(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "def sampling(model, xs):\n",
    "    model.update(xs)\n",
    "    return model.och_y.cws()\n",
    "\n",
    "mse_ftn = lambda model, xs, ys: mse_from_samples(ys, sampling(model, xs))\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "nll_metric = tf.keras.metrics.Mean(name='nll')\n",
    "\n",
    "test(du, x_test, y_test, batch, runtime_metric, (mse_ftn, mse_metric))\n",
    "\n",
    "template = 'DU Runtime: {} (ms), MSE: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DBNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBNN Runtime: 22.532424926757812 (ms), MSE: 1.326663613319397, NLL: 2.824204683303833\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "\n",
    "x_dims, y_dims = [AIR_INPUT_DIM], [AIR_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "dbnn = DBNN(lambda x: bnn_normal(bnn, x[0], AIR_REGULARIZER).sample(), och_x_1, och_x, och_y)\n",
    "\n",
    "def sampling(model, xs):\n",
    "    model.update(xs)\n",
    "    return [(c[0], w) for c, w in model.och_y.cws()]\n",
    "\n",
    "mse_ftn = lambda model, xs, ys: mse_from_samples(ys, sampling(model, xs))\n",
    "nll_ftn = lambda model, xs, ys: nll_from_samples(ys, sampling(model, xs), AIR_REGULARIZER)\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "nll_metric = tf.keras.metrics.Mean(name='nll')\n",
    "\n",
    "test(du, x_test, y_test, batch, runtime_metric, (mse_ftn, mse_metric), (nll_ftn, nll_metric))\n",
    "\n",
    "template = 'DBNN Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      nll_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "nll_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
