{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from dbnn.dbnn import DBNN\n",
    "from dbnn.och import OCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig = False\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "\n",
    "if not savefig:\n",
    "    plt.rcParams[\"figure.figsize\"] = (4, 4)\n",
    "    plt.rcParams[\"font.size\"] = 15\n",
    "    plt.rcParams[\"figure.titlesize\"] = 25\n",
    "    plt.rcParams[\"axes.labelsize\"] = 20\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "    plt.rcParams[\"legend.fontsize\"] = 13\n",
    "    plt.rcParams[\"lines.linewidth\"] = 2\n",
    "else:\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    plt.rcParams[\"font.size\"] = 30\n",
    "    plt.rcParams[\"axes.labelsize\"] = 53\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "    plt.rcParams[\"legend.fontsize\"] = 28\n",
    "    plt.rcParams[\"lines.linewidth\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants and hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "dataset_root = 'datasets/'\n",
    "dnn_path = 'models_checkpoints/dnn'\n",
    "bnn_path = 'models_checkpoints/bnn'\n",
    "\n",
    "# experiments\n",
    "seed = 30\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# och parameters\n",
    "och_x1_params = {'k': 5, 'l': 5.0, 's': 1.0}\n",
    "och_x_params = {'k': 5, 'l': 1.0, 's': 1.0}\n",
    "och_y_params = {'k': 5, 'l': 1.0, 's': 1.0}\n",
    "\n",
    "# style\n",
    "alpha = 0.12\n",
    "colors = [\"tab:blue\", \"tab:green\", \"tab:purple\", \"tab:red\"]\n",
    "labels = [\"DNN\", \"MU\", \"DU\", \"DBNN\"]\n",
    "guide_linestyle=(0, (1, 1))\n",
    "linestyles = [(0, (5, 1)), 'solid', (0, (5, 1)), 'solid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, x_train, y_train, batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    indexes_batch = np.array_split(np.random.permutation(len(x_train)), len(x_train) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "\n",
    "    for indexes in indexes_batch:\n",
    "        xs = tf.stack([x_train[index] for index in indexes])\n",
    "        ys = tf.stack([y_train[index] for index in indexes])\n",
    "        train_step(model, xs, ys, optimizer, loss_ftn, *loss_metrics)\n",
    "    \n",
    "def train_step(model, x_batch, y_batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    with tf.GradientTape() as tape:\n",
    "        losses = loss_ftn(model, x_batch, y_batch)\n",
    "    gradients = tape.gradient(losses[0], model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    for loss, loss_metric in zip(losses, loss_metrics):\n",
    "        loss_metric(loss)\n",
    "    \n",
    "def dnn_loss_ftn(model, xs, ys, loss_obj):\n",
    "    preds = model(xs)    \n",
    "    return loss_obj(ys, preds),\n",
    "\n",
    "def bnn_normal_loss_ftn(model, xs, ys, length):\n",
    "    logits = [tf.expand_dims(logit, axis=-1) for logit in tf.unstack(model(xs), axis=-1)]\n",
    "    nll = - tf.reduce_mean(tfd.Normal(loc=logits[0], scale=tf.math.softplus(logits[1])).log_prob(ys))\n",
    "    kl = sum(model.losses) / length\n",
    "    loss = nll + kl\n",
    "    return loss, nll\n",
    "\n",
    "def bnn_categorical_loss_ftn(model, xs, ys, length):\n",
    "    logits = model(xs)\n",
    "    nll = - tf.reduce_mean(tfd.Categorical(logits=logits).log_prob(ys))\n",
    "    kl = sum(model.losses) / length\n",
    "    loss = nll + kl\n",
    "    return loss, nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dnn_op(model, x):\n",
    "#     preds = model(tf.stack([x]))\n",
    "#     return preds\n",
    "\n",
    "def bnn_normal_op(model, x):\n",
    "    logits = tf.unstack(model(tf.stack([x])), axis=-1)\n",
    "    return tfd.Normal(loc=logits[0], scale=tf.math.softplus(logits[1])).sample()\n",
    "\n",
    "def bnn_categorical_op(model, x):\n",
    "    logits = model(x)\n",
    "    return tfd.Categorical(logits=logits).log_prob(ys).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "* [Toy Example of Regression](#Toy-Example-of-Regression)\n",
    "* [Occupancy Detection Data Set (Classification)](#Occupancy-Detection-Data-Set-(Classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example of Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOY_REGRESSION_FLAG = \"toy-regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "def create_bnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(10, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(2)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num = 50\n",
    "x_train = np.linspace(-3, 3, num=num)\n",
    "y_train = np.cos(x_train) + np.random.normal(0, 0.1, size=num)\n",
    "x_train = x_train.astype(np.float32).reshape((num, 1))\n",
    "y_train = y_train.astype(np.float32).reshape((num, 1))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.set_xlim(-5.2, 5.2)\n",
    "ax.set_ylim(-3.2, 3.2)\n",
    "ax.scatter(x_train, y_train, color=\"black\", s=30, marker=\"+\", label=\"Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 150, 10\n",
    "\n",
    "dnn = create_dnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_obj = tf.keras.losses.MSE\n",
    "loss_ftn = lambda model, xs, ys: dnn_loss_ftn(model, xs, ys, loss_obj)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(dnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        template = 'Epoch {}, Loss: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        \n",
    "dnn.save_weights(\"%s-%s\" % (dnn_path, TOY_REGRESSION_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs, batch = 300, 10\n",
    "\n",
    "bnn = create_bnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_ftn = lambda model, xs, ys: bnn_normal_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "        \n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        template = 'Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (bnn_path, TOY_REGRESSION_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(-5, 5, num=50)\n",
    "x_pred = x_pred.astype(np.float32).reshape((50, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = create_dnn()\n",
    "dnn.load_weights(\"%s-%s\" % (dnn_path, TOY_REGRESSION_FLAG))\n",
    "\n",
    "dnn_x_pred, dnn_y_pred, dnn_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    dnn_x_pred.append(x)\n",
    "    dnn_y_pred.append(dnn(tf.stack([x]))[0].numpy())\n",
    "    time2 = time.time()\n",
    "    dnn_runtimes.append(time2 - time1)\n",
    "    \n",
    "print(\"DNN Runtime: %.3f ± %.3f (ms)\" % (np.mean(dnn_runtimes) * 10 ** 3, np.std(dnn_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_no = 5\n",
    "bnn = create_bnn()\n",
    "bnn.load_weights(\"%s-%s\" % (bnn_path, TOY_REGRESSION_FLAG))\n",
    "\n",
    "mu_x_pred, mu_y_pred, mu_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    for _ in range(sample_no):\n",
    "        mu_pred = tf.unstack(bnn(tf.stack([x])), axis=-1)\n",
    "        mu_x_pred.append(x)\n",
    "        mu_y_pred.append(tfd.Normal(loc=mu_pred[0], scale=tf.math.softplus(mu_pred[1])).sample()[0].numpy())\n",
    "    time2 = time.time()\n",
    "    mu_runtimes.append(time2 - time1)\n",
    "    \n",
    "print(\"MU Runtime: %.3f ± %.3f (ms)\" % (np.mean(mu_runtimes) * 10 ** 3, np.std(mu_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_no = 5\n",
    "\n",
    "x_dims, y_dims = [1], [1]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "du = DBNN(lambda x: dnn(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "du_x_pred, du_y_pred, du_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    du.update(x)\n",
    "    time2 = time.time()\n",
    "    du_runtimes.append(time2 - time1)\n",
    "    for _ in range(sample_no):\n",
    "        _x_pred, _y_pred = du.och_x.sample(), du.och_y.sample()\n",
    "        if _x_pred is not None and _y_pred is not None:\n",
    "            du_x_pred.append(_x_pred[0])\n",
    "            du_y_pred.append(_y_pred.numpy())\n",
    "\n",
    "print(\"DU Runtime: %.3f ± %.3f (ms)\" % (np.mean(du_runtimes) * 10 ** 3, np.std(du_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_no = 5\n",
    "\n",
    "x_dims, y_dims = [1], [1]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "dbnn = DBNN(lambda x: bnn_normal_op(bnn, x[0]), och_x_1, och_x, och_y)\n",
    "\n",
    "dbnn_x_pred, dbnn_y_pred, dbnn_runtimes = [], [], []\n",
    "for x in x_pred:\n",
    "    time1 = time.time()\n",
    "    dbnn.update(x)\n",
    "    time2 = time.time()\n",
    "    dbnn_runtimes.append(time2 - time1)\n",
    "    for _ in range(sample_no):\n",
    "        _x_pred, _y_pred = dbnn.och_x.sample(), dbnn.och_y.sample()\n",
    "        if _x_pred is not None and _y_pred is not None:\n",
    "            dbnn_x_pred.append(_x_pred[0])\n",
    "            dbnn_y_pred.append(_y_pred.numpy())\n",
    "\n",
    "print(\"DBNN Runtime: %.3f ± %.3f (ms)\" % (np.mean(dbnn_runtimes) * 10 ** 3, np.std(dbnn_runtimes) * 10 ** 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18, 4))\n",
    "datasets = [(dnn_x_pred, dnn_y_pred), (mu_x_pred, mu_y_pred), (du_x_pred, du_y_pred), (dbnn_x_pred, dbnn_y_pred)]\n",
    "for ax, preds, label, color in zip(axes, datasets, [\"DNN\", \"MU\", \"DU\", \"DBNN\"], colors):\n",
    "    ax.set_xlim(-5.2, 5.2)\n",
    "    ax.set_ylim(-3.2, 3.2)\n",
    "    ax.scatter(x_train, y_train, color=\"black\", s=30, marker=\"+\", label=\"Training\")\n",
    "    ax.scatter(preds[0], preds[1], facecolors=\"none\", edgecolors=color, label=label)\n",
    "    ax.legend(edgecolor='black', fancybox=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Occupancy Detection Data Set](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCC_FLAG = \"occupancy\"\n",
    "OCC_INPUT_DIM = 5\n",
    "OCC_CLASS_NO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_file_names, test_file_names = [\"datatraining.txt\"], [\"datatest.txt\", \"datatest2.txt\"]\n",
    "\n",
    "def occupancy_data(file_names):\n",
    "    occupancy_path = dataset_root + \"occupancy/\"\n",
    "    x, y = [], []\n",
    "    for file_name in file_names:\n",
    "        with open(occupancy_path + file_name, 'r') as csvfile:\n",
    "            file_reader = csv.reader(csvfile, delimiter=',')\n",
    "            header = next(file_reader)\n",
    "            for row in file_reader:\n",
    "                x.append(tf.constant([float(v) for v in row[2: 7]]))\n",
    "                y.append(tf.constant(int(row[7])))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(OCC_CLASS_NO, activation=tf.nn.sigmoid)\n",
    "    ])\n",
    "\n",
    "def create_bnn():\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(50, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(50, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(OCC_CLASS_NO)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = occupancy_data(train_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 10, 10\n",
    "\n",
    "dnn = create_dnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_ftn = lambda model, xs, ys: dnn_loss_ftn(model, xs, ys, loss_obj)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(dnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = 'Epoch {}, Loss: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        \n",
    "dnn.save_weights(\"%s-%s\" % (dnn_path, OCC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch = 20, 10\n",
    "\n",
    "bnn = create_bnn()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = 'Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (bnn_path, OCC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset size:  12417\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = occupancy_data(test_file_names)\n",
    "\n",
    "print(\"Testset size: \", len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, xs, ys, sampling, runtime_metric, *loss_metrics):\n",
    "    for loss_ftn, metric in loss_metrics:\n",
    "        time1 = time.time()\n",
    "        sws = sampling(model, xs)\n",
    "        time2 = time.time()\n",
    "        runtime_metric(time2 - time1)\n",
    "        for loss_ftn, metric in loss_metrics:\n",
    "            losses = [tf.keras.losses.MSE(tf.one_hot(ys, OCC_CLASS_NO), s) * w for s, w in sws]\n",
    "            sum(losses)\n",
    "\n",
    "            loss = loss_ftn(sws, ys)\n",
    "            metric(loss)\n",
    "\n",
    "def test(model, x_test, y_test, batch, sampling, runtime_metric, *loss_metrics):\n",
    "    indexes_batch = np.array_split(range(len(x_test)), len(x_test) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "    for indexes in indexes_batch:\n",
    "        xs = tf.stack([x_test[index] for index in indexes])\n",
    "        ys = tf.stack([y_test[index] for index in indexes])\n",
    "        test_step(model, xs, ys, sampling, runtime_metric, *loss_metrics)\n",
    "        \n",
    "mse_ftn = lambda sws, ys: sum([tf.keras.losses.MSE(tf.one_hot(ys, OCC_CLASS_NO), s) * w for s, w in sws])\n",
    "cce_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "cce_ftn = lambda sws, ys: sum([cce_object(ys, s) * w for s, w, in sws])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x26905de0390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn = create_dnn()\n",
    "dnn.load_weights(\"%s-%s\" % (dnn_path, OCC_FLAG))\n",
    "\n",
    "bnn = create_bnn()\n",
    "bnn.load_weights(\"%s-%s\" % (bnn_path, OCC_FLAG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Runtime: 0.8591588735580444 (ms), MSE: 0.11299999803304672, NLL: 0.15665099024772644\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "\n",
    "sampling = lambda model, xs: [(model(xs), 1.0)]\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "test(dnn, x_test[:1000], y_test[:1000], batch, sampling, runtime_metric, (mse_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "        \n",
    "template = 'DNN Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 129.5395050048828, MSE: 0.25700029730796814, NLL: 4.1423563957214355\n"
     ]
    }
   ],
   "source": [
    "batch, sample_no = 1, 5\n",
    "\n",
    "sampling = lambda model, xs: [(tf.one_hot(tfd.Categorical(logits=model(xs)).sample(), OCC_CLASS_NO), 1 / sample_no) for _ in range(sample_no)]\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "test(bnn, x_test[:100], y_test[:100], batch, sampling, runtime_metric, (mse_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "\n",
    "template = 'MU Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 8.906378746032715, MSE: 0.45500001311302185, NLL: 0.630763053894043\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "\n",
    "x_dims, y_dims = [OCC_INPUT_DIM], [OCC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "du = DBNN(lambda x: dnn(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "def sampling(model, xs):\n",
    "    model.update(xs)\n",
    "    return model.och_y.cws()\n",
    "\n",
    "test(du, x_test[:100], y_test[:100], batch, sampling, runtime_metric, (mse_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "\n",
    "template = 'DU Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DBNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBNN Runtime: 11.921072006225586 (ms), MSE: 0.48750370740890503, NLL: 1.1915442943572998\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "\n",
    "x_dims, y_dims = [OCC_CLASS_NO], [OCC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "dbnn = DBNN(lambda x: bnn_categorical_op(bnn, x[0]), och_x_1, och_x, och_y)\n",
    "\n",
    "runtime_metric = tf.keras.metrics.Mean(name='runtime')\n",
    "mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "\n",
    "def sampling(model, xs):\n",
    "    model.update(xs)\n",
    "    return model.och_y.cws()\n",
    "\n",
    "test(du, x_test[:100], y_test[:100], batch, sampling, runtime_metric, (mse_ftn, mse_metric), (cce_ftn, cce_metric))\n",
    "\n",
    "template = 'DBNN Runtime: {} (ms), MSE: {}, NLL: {}'\n",
    "print(template.format(runtime_metric.result() * 1000, \n",
    "                      mse_metric.result(),\n",
    "                      cce_metric.result()))\n",
    "runtime_metric.reset_states()\n",
    "mse_metric.reset_states()\n",
    "cce_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [EMG Data for Gestures Data Set](https://archive.ics.uci.edu/ml/datasets/EMG+data+for+gestures) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_no = 8\n",
    "\n",
    "def emg_data(dataset_no):\n",
    "    emg_path = dataset_root + \"emg/\"\n",
    "    channels, labels = [], []\n",
    "    dat_path = emg_path + \"%02d/\" % dataset_no\n",
    "    for file_name in os.listdir(dat_path):\n",
    "        with open(dat_path + file_name, 'r') as csvfile:\n",
    "            file_reader = csv.reader(csvfile, delimiter='\\t')\n",
    "            header = next(file_reader)\n",
    "            for row in file_reader:\n",
    "                if len(row) == 10:\n",
    "                    channels.append(tf.constant([float(v) for v in row[1:8]]))\n",
    "                    labels.append(tf.constant([float(row[9])]))\n",
    "    return channels, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test DBNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
