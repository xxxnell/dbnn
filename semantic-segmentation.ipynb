{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import models as models\n",
    "import ops.semantic_segmentation.datasets as datasets\n",
    "import ops.semantic_segmentation.imageops as imageops\n",
    "import ops.semantic_segmentation.trains as trains\n",
    "import ops.semantic_segmentation.tests as tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "cwd = os.getcwd()\n",
    "model_path = 'models_checkpoints'\n",
    "dat_path = 'leaderboard/semantic-segmentation'\n",
    "\n",
    "# CamVid\n",
    "dataset_name = 'camvid'\n",
    "img_size = 720 // 2, 960 // 2\n",
    "crop_size = 720 // 2, 960 // 2\n",
    "dataset_root = '%s/datasets/camvid' % cwd\n",
    "seq_root = '%s/datasets/camvid/seq' % cwd\n",
    "\n",
    "# CityScape\n",
    "dataset_name = 'cityscape'\n",
    "img_size = 1024 // 2, 2048 // 2\n",
    "crop_size = 1024 // 2, 2048 // 2\n",
    "dataset_root = '%s/datasets/cityscape' % cwd\n",
    "seq_root = '%s/datasets/cityscape' % cwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load label informations, namely color infos and number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = datasets.colors(dataset_name)\n",
    "num_classes = len(set(colors.values()))\n",
    "print('%d classes are loaded. ' % num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "offset = (5, 0)\n",
    "dataset_train, dataset_val, dataset_test = datasets.dataset(\n",
    "    dataset_name, dataset_root, img_size, crop_size, cache=True)\n",
    "dataset_seq = datasets.dataset_seq(\n",
    "    dataset_name, dataset_root, seq_root, img_size, offset=offset)\n",
    "dataset_test = dataset_seq.map(lambda image, label: (image[offset[0]], label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `offset` indicates how many past and future frames (5 and 0 respectively in this example) to use when evaluating the model by using the video stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since senamtic segmentation datasets such as CamVid and CityScape are imbalanced, we use Median Frequency Balancing to correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class_weights = datasets.median_freq_weights(dataset_train, num_classes)  # manually calculates weights\n",
    "class_weights = datasets.memorized_median_freq_weights(dataset_name)  # load memorized weights (cost efficient)\n",
    "print('Class weights: \\n', class_weights.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use U-Net and SegNet with MC dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Deterministic NN\n",
    "# model = models.UNet(num_classes, name='u-net-dnn')\n",
    "# model = models.SegNet(num_classes, name='segnet-dnn')\n",
    "\n",
    "# Bayesian NN (MC dropout)\n",
    "model = models.UNet(num_classes, rate=0.5, name='u-net-bnn')\n",
    "# model = models.SegNet(num_classes, rate=0.5, name='segnet-bnn')\n",
    "\n",
    "# model.load_weights('%s/%s_%s' % (model_path, dataset_name, model.name + '_1')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load TensorBoard variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = 'logs/gradient_tape/%s_%s/%s' % (dataset_name, model.name, current_time)\n",
    "train_log_dir = '%s/train' % log_dir\n",
    "test_log_dir = '%s/test' % log_dir\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "print('Create TensorBoard Log dir: ', log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "epochtime_metric = tf.keras.metrics.Mean(name='epoch_time')\n",
    "loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_metric = tf.keras.metrics.Mean(name='train_nll')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_time = time.time()\n",
    "    loss, nll = trains.train_epoch(optimizer, model, dataset_train, num_classes, class_weights, batch_size=3)\n",
    "    epochtime_metric(time.time() - batch_time)\n",
    "    loss_metric(loss)\n",
    "    nll_metric(nll)\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = '(%.2f sec) Epoch: %d, Loss: %.4f, NLL: %.4f'\n",
    "        print(template % (epochtime_metric.result(),\n",
    "                          epoch,\n",
    "                          loss_metric.result(),\n",
    "                          nll_metric.result()))\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss_metric.result(), step=epoch)\n",
    "            tf.summary.scalar('nll', nll_metric.result(), step=epoch)\n",
    "        \n",
    "        epochtime_metric.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        nll_metric.reset_states()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        metrics = tests.test_sampling(model, 5, \n",
    "                                      dataset_val, num_classes, \n",
    "                                      batch_size=1, cutoffs=(0.0, 0.9), verbose=False)\n",
    "        \n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('nll', metrics[0], step=epoch)\n",
    "            tf.summary.scalar('iou', metrics[2][0], step=epoch)\n",
    "            tf.summary.scalar('iou-90', metrics[2][1], step=epoch)\n",
    "            tf.summary.scalar('acc', metrics[3][0], step=epoch)\n",
    "            tf.summary.scalar('acc-90', metrics[3][1], step=epoch)\n",
    "            tf.summary.scalar('unc-90', metrics[4][1], step=epoch)\n",
    "            tf.summary.scalar('freq-90', metrics[5][1], step=epoch)\n",
    "            tf.summary.scalar('ece', metrics[9], step=epoch)\n",
    "            tf.summary.image('calibration diagrams', metrics[10], step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('%s/%s_%s' % (model_path, dataset_name, model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following operation evaluates the model by using _deterministic neural network (DNN)_ prediction, i.e., $p(\\textbf{y} \\vert \\textbf{x}_0, \\textbf{w}_0)$ for an observed input $\\textbf{x}_0$ and one execution $\\textbf{w}_0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tests.test_vanilla(model,\n",
    "                       dataset_test, num_classes, batch_size=3, \n",
    "                       cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following operation evaluates the model by using predictive distributon of _Bayesian neural network (BNN)_ with MC estimator, i.e., average of results with several forward passes $p(\\textbf{y} \\vert \\textbf{x}_0, \\mathcal{D}) \\simeq \\sum_{\\textbf{w}_i} \\frac{1}{N} p(\\textbf{y} \\vert \\textbf{x}_0, \\textbf{w}_i)$ where $p(\\textbf{y} \\vert \\textbf{x}_0, \\textbf{w}_i)$ is the neural network prediction for an observed input $\\textbf{x}_0$ and different executions $\\textbf{w}_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tests.test_sampling(model, 30, \n",
    "                        dataset_test, num_classes, batch_size=3, \n",
    "                        cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use 30 samples. Since semantic segmentation is a classification task, the prediction $p(\\textbf{y} \\vert \\textbf{x}_0, \\textbf{w}_i)$ is a softmax of NN logit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following operation evaluates the model by using predictive distribution of _vector quantized Bayesian neural network (VQ-BNN)_ for data stream, i.e., _temporal smoothing_ or _exponential moving average (EMA)_ of recent predictions $p(\\textbf{y} \\vert \\textbf{x}_0, \\mathcal{D}) \\simeq \\sum_{t=0}^{-K} \\pi(\\textbf{x}_t \\vert \\mathcal{S}) \\, p(\\textbf{y} \\vert \\textbf{x}_t, \\textbf{w}_t)$ where $\\{ \\textbf{x}_0, \\textbf{x}_{-1}, \\cdots \\}$ are recent video frames, $p(\\textbf{y} \\vert \\textbf{x}_t, \\textbf{w}_t)$ are recent NN predictions, and $\\pi(\\textbf{x}_t \\vert \\mathcal{S}) = \\frac{\\exp(- \\vert t \\vert / \\tau )}{\\sum_{0}^{K} \\exp(- \\vert t \\vert / \\tau)}$ are exponentially decaying importances of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tests.test_temporal_smoothing(model, 0.8, offset,\n",
    "                                  dataset_seq, num_classes, batch_size=3, \n",
    "                                  cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use $\\tau=0.8$ and $K=5$ (`offset[0]`) in this example. We can further improve the predictive performance of VQ-BNN by setting `offset[1]` to 1 or more when loading the data sequence. This means that we use the future prediction as well as the past prediction for VQ-BNN inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
