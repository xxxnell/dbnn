{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import models as models\n",
    "import ops.load_scene_datasets as datasets\n",
    "import ops.imageops as imageops\n",
    "import ops.trains as trains\n",
    "import ops.tests as tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['font.size'] = 15\n",
    "plt.rcParams['figure.titlesize'] = 25\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['legend.fontsize'] = 13\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "# plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "# plt.rcParams['figure.figsize'] = (8, 8)\n",
    "# plt.rcParams['font.size'] = 30\n",
    "# plt.rcParams['axes.labelsize'] = 48\n",
    "# plt.rcParams['xtick.labelsize'] = 40\n",
    "# plt.rcParams['ytick.labelsize'] = 40\n",
    "# plt.rcParams['legend.fontsize'] = 28\n",
    "# plt.rcParams['lines.linewidth'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "cwd = os.getcwd()\n",
    "model_path = 'models_checkpoints'\n",
    "dat_path = 'leaderboard/semantic-segmentation'\n",
    "\n",
    "# CamVid\n",
    "dataset_name = 'camvid'\n",
    "img_size = 720 // 2, 960 // 2\n",
    "crop_size = 720 // 2, 960 // 2\n",
    "dataset_root = '%s/datasets/camvid' % cwd\n",
    "seq_root = 'F:/research/dataset/camvid/seq'\n",
    "\n",
    "# CityScape\n",
    "dataset_name = 'cityscape'\n",
    "img_size = 1024 // 2, 2048 // 2\n",
    "crop_size = 480, 560\n",
    "dataset_root = 'F:/research/dataset/cityscape'\n",
    "seq_root = 'F:/research/dataset/cityscape'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load label informations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = datasets.colors(dataset_name)\n",
    "num_classes = len(set(colors.values()))\n",
    "print('%d classes are loaded. ' % num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "offset = (10, 0)\n",
    "dataset_train, dataset_val, dataset_test = datasets.dataset(\n",
    "    dataset_name, dataset_root, img_size, crop_size)\n",
    "dataset_seq = datasets.dataset_seq(\n",
    "    dataset_name, dataset_root, seq_root, img_size, offset=offset)\n",
    "dataset_test = dataset_seq.map(lambda image, label: (image[offset[0] - 1], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class_weights = datasets.median_freq_weights(dataset_train, num_classes)  # manually calculates weights\n",
    "class_weights = datasets.memorized_median_freq_weights(dataset_name)  # load memorized weights\n",
    "print('Class weights: \\n', class_weights.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DNN\n",
    "model = models.UNet(num_classes, name='u-net-dnn')\n",
    "# model = models.SegNet(num_classes, name='segnet-dnn')\n",
    "\n",
    "# BNN\n",
    "# model = models.UNet(num_classes, rate=0.5, name='u-net-bnn')\n",
    "# model = models.SegNet(num_classes, rate=0.5, name='segnet-bnn')\n",
    "\n",
    "# model.load_weights(''%s/%s_%s' % (model_path, dataset_name, model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "log_dir = 'logs/gradient_tape/%s_%s/%s' % (dataset_name, model.name, current_time)\n",
    "train_log_dir = '%s/train' % log_dir\n",
    "test_log_dir = '%s/test' % log_dir\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "    \n",
    "print('Create TensorBoard Log dir: ', log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "epochtime_metric = tf.keras.metrics.Mean(name='epoch_time')\n",
    "loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_metric = tf.keras.metrics.Mean(name='train_nll')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_time = time.time()\n",
    "    loss, nll = trains.train_epoch(optimizer, model, dataset_train, num_classes, class_weights, batch_size=3)\n",
    "    epochtime_metric(time.time() - batch_time)\n",
    "    loss_metric(loss)\n",
    "    nll_metric(nll)\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = '(%.2f sec) Epoch: %d, Loss: %.4f, NLL: %.4f'\n",
    "        print(template % (epochtime_metric.result(),\n",
    "                          epoch,\n",
    "                          loss_metric.result(),\n",
    "                          nll_metric.result()))\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss_metric.result(), step=epoch)\n",
    "            tf.summary.scalar('nll', nll_metric.result(), step=epoch)\n",
    "        \n",
    "        epochtime_metric.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        nll_metric.reset_states()\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        metrics = tests.test_sampling(model, 1, \n",
    "                                      dataset_test, num_classes, \n",
    "                                      batch_size=3, cutoffs=(0.0, 0.9), verbose=False)\n",
    "        \n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('nll', metrics[0], step=epoch)\n",
    "            tf.summary.scalar('iou', metrics[2][0], step=epoch)\n",
    "            tf.summary.scalar('iou-90', metrics[2][1], step=epoch)\n",
    "            tf.summary.scalar('acc', metrics[3][0], step=epoch)\n",
    "            tf.summary.scalar('acc-90', metrics[3][1], step=epoch)\n",
    "            tf.summary.scalar('unc-90', metrics[4][1], step=epoch)\n",
    "            tf.summary.scalar('cov-90', metrics[5][1], step=epoch)\n",
    "            tf.summary.scalar('ece', metrics[9], step=epoch)\n",
    "            tf.summary.image('calibration diagrams', metrics[10], step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('%s/%s_%s' % (model_path, dataset_name, model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = tests.test_vanilla(model, \n",
    "                       dataset_test, num_classes, batch_size=3, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tests.test_sampling(model, 10, \n",
    "                        dataset_test, num_classes, batch_size=3, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = tests.test_temporal_smoothing(model, 1.0, \n",
    "                                  dataset_seq, num_classes, batch_size=3, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dims, y_dims = [img_size[0] * img_size[1] * img_size[2]], [img_size[0] * img_size[1] * 1]\n",
    "och_x = OCH(k=10, l=1.3, s=1.0, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(k=10, l=1.3, s=1.0, dims=y_dims, hash_no=1, ann='argmax')\n",
    "vqbnn = VQBNN(lambda x: model(tf.expand_dims(x, axis=0)), och_x=och_x, och_y=och_y, posterior=None)\n",
    "\n",
    "_ = tests.test_vq(vqbnn, \n",
    "                  dataset_seq, num_classes, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in dataset_test.shuffle(10).batch(1).take(1):\n",
    "    ys_pred = tests.predict_sampling(model, xs, 10)\n",
    "    ys_pred, unc_pred = tf.math.argmax(ys_pred, axis=-1), tf.math.reduce_max(ys_pred, axis=-1)\n",
    "    \n",
    "    ys = imageops.to_color(ys, colors)\n",
    "    ys_pred = imageops.to_color(ys_pred, colors)\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "    for ax, image in zip(axes, [xs[0], ys[0], ys_pred[0], unc_pred[0]]):\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in dataset_seq.shuffle(10).batch(1).take(1):\n",
    "    ys_pred = tests.predict_temporal_smoothing(model, xs, 1.0)\n",
    "    ys_pred, unc_pred = tf.math.argmax(ys_pred, axis=-1), tf.math.reduce_max(ys_pred, axis=-1)\n",
    "    \n",
    "    ys = imageops.to_color(ys, colors)\n",
    "    ys_pred = imageops.to_color(ys_pred, colors)\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "    for ax, image in zip(axes, [xs[0, -1], ys[0], ys_pred[0], unc_pred[0]]):\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
