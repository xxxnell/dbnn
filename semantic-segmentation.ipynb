{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import semantic_segmentation.models as models\n",
    "import semantic_segmentation.ops.datasets as datasets\n",
    "import semantic_segmentation.ops.imageops as imageops\n",
    "import semantic_segmentation.ops.trains as trains\n",
    "import semantic_segmentation.ops.tests as tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "plt.rcParams[\"figure.figsize\"] = (4, 4)\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "plt.rcParams[\"figure.titlesize\"] = 25\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "plt.rcParams[\"legend.fontsize\"] = 13\n",
    "plt.rcParams[\"lines.linewidth\"] = 2\n",
    "\n",
    "# plt.rcParams[\"font.family\"] = \"serif\"\n",
    "# plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "# plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "# plt.rcParams[\"font.size\"] = 30\n",
    "# plt.rcParams[\"axes.labelsize\"] = 48\n",
    "# plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "# plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "# plt.rcParams[\"legend.fontsize\"] = 28\n",
    "# plt.rcParams[\"lines.linewidth\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "cwd = os.getcwd()\n",
    "model_path = \"models_checkpoints\"\n",
    "dat_path = \"leaderboard/semantic-segmentation\"\n",
    "\n",
    "# CamVid\n",
    "dataset_name = 'camvid'\n",
    "img_size = 720 // 2, 960 // 2\n",
    "crop_size = 720 // 2, 960 // 2\n",
    "dataset_root = '%s/datasets/camvid' % cwd\n",
    "seq_root = 'F:/research/dataset/camvid/seq'\n",
    "\n",
    "# CityScape\n",
    "# dataset_name = 'cityscape'\n",
    "# img_size = 1024 // 2, 2048 // 2\n",
    "# crop_size = 480, 560\n",
    "# dataset_root = 'F:/research/dataset/cityscape'\n",
    "# seq_root = 'F:/research/dataset/cityscape'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load label informations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 classes are loaded. \n"
     ]
    }
   ],
   "source": [
    "colors = datasets.colors(dataset_name)\n",
    "num_classes = len(set(colors.values()))\n",
    "print(\"%d classes are loaded. \" % num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "offset = (10, 0)\n",
    "dataset_train, dataset_val, dataset_test = datasets.dataset(\n",
    "    dataset_name, dataset_root, img_size, crop_size)\n",
    "dataset_seq = datasets.dataset_seq(\n",
    "    dataset_name, dataset_root, seq_root, img_size, offset=offset)\n",
    "dataset_test = dataset_seq.map(lambda image, label: (image[offset[0] - 1], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: \n",
      " [0.30734012 0.19833793 4.7175865  0.16562003 0.6806351  0.42397258\n",
      " 4.2133756  3.256359   1.         6.7325764  9.058633  ]\n"
     ]
    }
   ],
   "source": [
    "# class_weights = datasets.median_freq_weights(dataset_train, num_classes)  # manually calculates weights\n",
    "class_weights = datasets.memorized_median_freq_weights(dataset_name)  # load memorized weights\n",
    "print(\"Class weights: \\n\", class_weights.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DNN\n",
    "model = models.UNet(num_classes, name='u-net-dnn')\n",
    "# model = models.SegNet(num_classes, name='segnet-dnn')\n",
    "\n",
    "# BNN\n",
    "# model = models.UNet(num_classes, rate=0.5, name='u-net-bnn')\n",
    "# model = models.SegNet(num_classes, rate=0.5, name='segnet-bnn')\n",
    "\n",
    "# model.load_weights(\"%s/%s_%s\" % (model_path, dataset_name, model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create TensorBoard Log dir:  logs/gradient_tape/camvid_u-net-dnn/20200801-165813\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/gradient_tape/%s_%s/%s' % (dataset_name, model.name, current_time)\n",
    "train_log_dir = '%s/train' % log_dir\n",
    "test_log_dir = '%s/test' % log_dir\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "    \n",
    "print('Create TensorBoard Log dir: ', log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "epochtime_metric = tf.keras.metrics.Mean(name='epoch_time')\n",
    "loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_metric = tf.keras.metrics.Mean(name='train_nll')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_time = time.time()\n",
    "    loss, nll = trains.train_epoch(optimizer, model, dataset_train, num_classes, class_weights, batch_size=3)\n",
    "    epochtime_metric(time.time() - batch_time)\n",
    "    loss_metric(loss)\n",
    "    nll_metric(nll)\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = '(%.2f sec) Epoch: %d, Loss: %.4f, NLL: %.4f'\n",
    "        print(template % (epochtime_metric.result(),\n",
    "                          epoch,\n",
    "                          loss_metric.result(),\n",
    "                          nll_metric.result()))\n",
    "        \n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', loss_metric.result(), step=epoch)\n",
    "            tf.summary.scalar('nll', nll_metric.result(), step=epoch)\n",
    "        \n",
    "        epochtime_metric.reset_states()\n",
    "        loss_metric.reset_states()\n",
    "        nll_metric.reset_states()\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        metrics = tests.test_sampling(model, 10, \n",
    "                                      dataset_test, num_classes, \n",
    "                                      batch_size=3, cutoffs=(0.0, 0.9), verbose=False)\n",
    "        \n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('nll', metrics[0], step=epoch)\n",
    "            tf.summary.scalar('iou', metrics[2][0], step=epoch)\n",
    "            tf.summary.scalar('iou-90', metrics[2][1], step=epoch)\n",
    "            tf.summary.scalar('acc', metrics[3][0], step=epoch)\n",
    "            tf.summary.scalar('acc-90', metrics[3][1], step=epoch)\n",
    "            tf.summary.scalar('unc-90', metrics[4][1], step=epoch)\n",
    "            tf.summary.scalar('cov-90', metrics[5][1], step=epoch)\n",
    "            tf.summary.scalar('ece', metrics[9], step=epoch)\n",
    "            tf.summary.image('calibration diagrams', metrics[10], step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"%s/%s_%s\" % (model_path, dataset_name, model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = tests.test_vanilla(model, \n",
    "                       dataset_test, num_classes, batch_size=3, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tests.test_sampling(model, 10, \n",
    "                        dataset_test, num_classes, batch_size=3, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = tests.test_temporal_smoothing(model, 1.0, \n",
    "                                  dataset_seq, num_classes, batch_size=3, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "och_x_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "_ = tests.test_vq(model, och_x_params, och_y_params, \n",
    "                  dataset_seq, num_classes, cutoffs=(0.0, 0.7, 0.9), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in dataset_test.shuffle(1).batch(1).take(1):\n",
    "    ys_pred = tests.predict_sampling(model, xs, 10)\n",
    "    ys_pred, unc_pred = tf.math.argmax(ys_pred, axis=-1), tf.math.reduce_max(ys_pred, axis=-1)\n",
    "    \n",
    "    ys = imageops.to_color(ys, colors)\n",
    "    ys_pred = imageops.to_color(ys_pred, colors)\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "    for ax, image in zip(axes, [xs[0], ys[0], ys_pred[0], unc_pred[0]]):\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xs, ys in dataset_seq.shuffle(1).batch(1).take(1):\n",
    "    ys_pred = tests.predict_temporal_smoothing(model, xs, 1.0)\n",
    "    ys_pred, unc_pred = tf.math.argmax(ys_pred, axis=-1), tf.math.reduce_max(ys_pred, axis=-1)\n",
    "    \n",
    "    ys = imageops.to_color(ys, colors)\n",
    "    ys_pred = imageops.to_color(ys_pred, colors)\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "    for ax, image in zip(axes, [xs[0, -1], ys[0], ys_pred[0], unc_pred[0]]):\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
