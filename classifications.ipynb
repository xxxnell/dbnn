{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from vqbnn.vqbnn import VQBNN\n",
    "from vqbnn.och import OCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig = False\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "\n",
    "if not savefig:\n",
    "    plt.rcParams[\"figure.figsize\"] = (4, 4)\n",
    "    plt.rcParams[\"font.size\"] = 15\n",
    "    plt.rcParams[\"figure.titlesize\"] = 25\n",
    "    plt.rcParams[\"axes.labelsize\"] = 20\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "    plt.rcParams[\"legend.fontsize\"] = 13\n",
    "    plt.rcParams[\"lines.linewidth\"] = 2\n",
    "else:\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    plt.rcParams[\"font.size\"] = 30\n",
    "    plt.rcParams[\"axes.labelsize\"] = 53\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "    plt.rcParams[\"legend.fontsize\"] = 28\n",
    "    plt.rcParams[\"lines.linewidth\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants and hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "CLASSIFICATION_FLAG = \"classifications\"\n",
    "DATASET_PATH = 'datasets/'\n",
    "BNN_PATH = 'models_checkpoints/bnn'\n",
    "DAT_PATH = \"leaderboard/%s/\" % CLASSIFICATION_FLAG\n",
    "\n",
    "# experiments\n",
    "seed = 30\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "POSTERIOR_NO = 30\n",
    "CONFIDENCE_CUTOFF = 0.9\n",
    "\n",
    "# och parameters\n",
    "och_x1_params = {'k': 5, 'l': 5.0, 's': 1.0}\n",
    "och_x_params = {'k': 10, 'l': 0.01, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 0.01, 's': 1.0}\n",
    "\n",
    "# style\n",
    "alpha = 0.12\n",
    "colors = [\"tab:blue\", \"tab:green\", \"tab:purple\", \"tab:red\"]\n",
    "labels = [\"DNN\", \"MU\", \"DU\", \"VQ-BNN\"]\n",
    "guide_linestyle=(0, (1, 1))\n",
    "linestyles = [(0, (5, 1)), 'solid', (0, (5, 1)), 'solid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_categorical(model, xs):\n",
    "    logits = model(xs)\n",
    "    return tfd.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, x_train, y_train, batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    indexes_batch = np.array_split(np.random.permutation(len(x_train)), len(x_train) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "\n",
    "    for indexes in indexes_batch:\n",
    "        xs = tf.stack([x_train[index] for index in indexes])\n",
    "        ys = tf.stack([y_train[index] for index in indexes])\n",
    "        train_step(model, xs, ys, optimizer, loss_ftn, *loss_metrics)\n",
    "    \n",
    "def train_step(model, x_batch, y_batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    with tf.GradientTape() as tape:\n",
    "        losses = loss_ftn(model, x_batch, y_batch)\n",
    "    gradients = tape.gradient(losses[0], model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    for loss, loss_metric in zip(losses, loss_metrics):\n",
    "        loss_metric(loss)\n",
    "        \n",
    "def bnn_categorical_loss_ftn(model, xs, ys, length):\n",
    "    ys = tf.squeeze(ys, axis=-1)\n",
    "    nll = - tf.reduce_mean(bnn_categorical(model, xs).log_prob(ys))\n",
    "    kl = sum(model.losses) / length\n",
    "    loss = nll + kl\n",
    "    return loss, nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(model, x_test, y_test, depth, batch=1, stat_period=1, sample_no=POSTERIOR_NO, cutoff=CONFIDENCE_CUTOFF, print_stat=False):\n",
    "    cce_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "    mse_90_metric = tf.keras.metrics.Mean(name='mse-90')\n",
    "    cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "    cov_metric = tf.keras.metrics.Mean(name='cov')\n",
    "\n",
    "    indexes_batch = np.array_split(range(len(x_test)), len(x_test) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "    for i, indexes in enumerate(indexes_batch):\n",
    "        start_time = time.time()\n",
    "        xs = tf.stack([x_test[index] for index in indexes])\n",
    "        ys = tf.stack([y_test[index] for index in indexes])\n",
    "        ys = tf.one_hot(tf.reshape(ys, [-1]), depth)\n",
    "        \n",
    "        prediction = [model(xs) for _ in range(sample_no)]\n",
    "        prediction = tf.nn.softmax(prediction)\n",
    "        prediction = tf.math.reduce_mean(prediction, axis=0)\n",
    "        filtered = [(p, y) for p, y in zip(prediction, ys) if tf.math.reduce_max(p) >= cutoff]\n",
    "        prediction_cutoff, ys_cutoff = tf.stack([p for p, _ in filtered]), tf.stack([y for _, y in filtered])\n",
    "        \n",
    "        if print_stat:\n",
    "            print(\"ys:\", np.array([y_test[index][0] for index in indexes]))\n",
    "            print(\"result: \", tf.math.argmax(prediction, axis=-1))\n",
    "            print(\"confidence: \", tf.math.reduce_max(prediction, axis=-1))\n",
    "        \n",
    "        mse = tf.keras.losses.MSE(ys, prediction)\n",
    "        cce = cce_object(ys, prediction)\n",
    "        mse_cutoff = tf.keras.losses.MSE(ys_cutoff, prediction_cutoff)\n",
    "\n",
    "        mse_metric(mse)\n",
    "        cce_metric(cce)\n",
    "        cov_metric(len(prediction_cutoff) / len(prediction))\n",
    "        if len(filtered) > 0:\n",
    "            mse_90_metric(mse_cutoff)\n",
    "    \n",
    "        if ((i + 1) % stat_period is 0) or ((i + 1) is len(indexes_batch)):\n",
    "            template = \"(%s, %.1f sec/epoch), Epoch %d/%d, MSE: %.4f, MSE-90: %.4f, CCE: %.4f, Cov-90: %.4f %%\"\n",
    "            print(template % (datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                              (time.time() - start_time) / stat_period,\n",
    "                              i + 1,\n",
    "                              len(indexes_batch),\n",
    "                              mse_metric.result(),\n",
    "                              mse_90_metric.result(),\n",
    "                              cce_metric.result(),\n",
    "                              cov_metric.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vqbnn(vqbnn, x_test, y_test, depth, stat_period=1, sample_no=POSTERIOR_NO, cutoff=CONFIDENCE_CUTOFF):\n",
    "    cce_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "    mse_90_metric = tf.keras.metrics.Mean(name='mse-90')\n",
    "    cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "    cov_metric = tf.keras.metrics.Mean(name='cov')\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(x_test, y_test)):\n",
    "        start_time = time.time()\n",
    "        ys = tf.one_hot(y, depth)\n",
    "        vqbnn.update(x)\n",
    "        \n",
    "        if sum([w for _, w in vqbnn.och_y.cws()]) > 0:\n",
    "            prediction = [tf.nn.softmax(c) * w for c, w in vqbnn.och_y.cws()]\n",
    "            prediction = tf.math.add_n(prediction)\n",
    "            filtered = [(p, y) for p, y in zip(prediction, ys) if tf.math.reduce_max(p) >= cutoff]\n",
    "            prediction_cutoff, ys_cutoff = tf.stack([p for p, _ in filtered]), tf.stack([y for _, y in filtered])\n",
    "\n",
    "            mse = tf.keras.losses.MSE(ys, prediction)\n",
    "            cce = cce_object(ys, prediction)\n",
    "            mse_cutoff = tf.keras.losses.MSE(ys_cutoff, prediction_cutoff)\n",
    "            \n",
    "            mse_metric(mse)\n",
    "            cov_metric(len(prediction_cutoff) / len(prediction))\n",
    "            cce_metric(cce)\n",
    "            if len(filtered) > 0:\n",
    "                mse_90_metric(mse_cutoff)\n",
    "    \n",
    "        if ((i + 1) % stat_period is 0) or ((i + 1) is len(x_test)):\n",
    "            template = \"(%s, %.1f sec/epoch), Epoch %d/%d, MSE: %.4f, MSE-90: %.4f, CCE: %.4f, Cov-90: %.4f\"\n",
    "            print(template % (datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                              (time.time() - start_time) / stat_period,\n",
    "                              i + 1,\n",
    "                              len(x_test),\n",
    "                              mse_metric.result(),\n",
    "                              mse_90_metric.result(),\n",
    "                              cce_metric.result(),\n",
    "                              cov_metric.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_NAMES = DENSE_1, DENSE_2, DENSE_3 = \"dense_1\", \"dense_2\", \"dense_3\"\n",
    "\n",
    "def create_dnn(classes_no, unit=50):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(unit, activation=tf.nn.relu, name=DENSE_1),\n",
    "        tf.keras.layers.Dense(unit, activation=tf.nn.relu, name=DENSE_2),\n",
    "        tf.keras.layers.Dense(unit, activation=tf.nn.relu, name=DENSE_3),\n",
    "        tf.keras.layers.Dense(classes_no, name=DENSE_3)\n",
    "    ])\n",
    "\n",
    "def create_bnn(classes_no, unit=50):\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(unit, activation=tf.nn.relu, name=DENSE_1),\n",
    "        tfp.layers.DenseFlipout(unit, activation=tf.nn.relu, name=DENSE_2),\n",
    "        tfp.layers.DenseFlipout(classes_no, name=DENSE_3)\n",
    "    ])\n",
    "\n",
    "def freeze(bnn, input_dim, posterior_no):\n",
    "    kernel_d1, bias_d1 = [], []\n",
    "    kernel_d2, bias_d2 = [], []\n",
    "    kernel_d3, bias_d3 = [], []\n",
    "    \n",
    "    layer1 = bnn.get_layer(DENSE_1) \n",
    "    layer2 = bnn.get_layer(DENSE_2) \n",
    "    layer3 = bnn.get_layer(DENSE_3) \n",
    "\n",
    "    layer1.kernel_posterior_tensor_fn = lambda d: append(kernel_d1, d.sample())\n",
    "    layer1.bias_posterior_tensor_fn = lambda d: append(bias_d1, d.sample())\n",
    "    layer2.kernel_posterior_tensor_fn = lambda d: append(kernel_d2, d.sample())\n",
    "    layer2.bias_posterior_tensor_fn = lambda d: append(bias_d2, d.sample())\n",
    "    layer3.kernel_posterior_tensor_fn = lambda d: append(kernel_d3, d.sample())\n",
    "    layer3.bias_posterior_tensor_fn = lambda d: append(bias_d3, d.sample())\n",
    "        \n",
    "    _ = [bnn(tf.random.normal((1, input_dim))) for _ in range(posterior_no)]\n",
    "    \n",
    "    layer1.kernel_posterior_tensor_fn = lambda d: kernel_d1[random.randint(0, posterior_no - 1)]\n",
    "    layer1.bias_posterior_tensor_fn = lambda d: bias_d1[random.randint(0, posterior_no - 1)]\n",
    "    layer2.kernel_posterior_tensor_fn = lambda d: kernel_d2[random.randint(0, posterior_no - 1)]\n",
    "    layer2.bias_posterior_tensor_fn = lambda d: bias_d2[random.randint(0, posterior_no - 1)]\n",
    "    layer3.kernel_posterior_tensor_fn = lambda d: kernel_d3[random.randint(0, posterior_no - 1)]\n",
    "    layer3.bias_posterior_tensor_fn = lambda d: bias_d3[random.randint(0, posterior_no - 1)]\n",
    "    \n",
    "def append(xs, x):\n",
    "    xs.append(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "* [Occupancy Detection Data Set (Classification)](#Occupancy-Detection-Data-Set-(Classification))\n",
    "* [EMG Data for Gestures Data Set (Classification)](#EMG-Data-for-Gestures-Data-Set-(Classification))\n",
    "* [Localization Data for Person Activity Data Set (Classification)](#Localization-Data-for-Person-Activity-Data-Set-(Classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Occupancy Detection Data Set](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCC_FLAG = \"occupancy\"\n",
    "OCC_INPUT_DIM = 5\n",
    "OCC_CLASS_NO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_file_names, test_file_names = [\"datatraining.txt\"], [\"datatest.txt\", \"datatest2.txt\"]\n",
    "\n",
    "def occupancy_data(file_name):\n",
    "    occupancy_path = DATASET_PATH + \"%s/\" % OCC_FLAG\n",
    "    x, y = [], []\n",
    "    with open(occupancy_path + file_name, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            x.append(tf.constant([float(v) for v in row[2: 7]]))\n",
    "            y.append(tf.constant([int(row[7])]))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = occupancy_data(train_file_names[0])\n",
    "x_test, y_test = tuple(zip(*[occupancy_data(test_file_name) for test_file_name in test_file_names]))\n",
    "x_test, y_test = tf.concat(x_test, 0), tf.concat(y_test, 0)\n",
    "\n",
    "print(\"Train dataset: %d, Testset size: %d\" % (len(x_train), len(x_test)))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(OCC_CLASS_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch = 50, 100\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "    time2 = time.time()\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = '(%.2f sec) Epoch: %d, Loss: %.4f, NLL: %.4f'\n",
    "        print(template % (time2 - time1,\n",
    "                          epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (BNN_PATH, OCC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(OCC_CLASS_NO)\n",
    "bnn.load_weights(\"%s-%s\" % (BNN_PATH, OCC_FLAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_nn(bnn, x_test, y_test, OCC_CLASS_NO, batch=1, stat_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test VQ-BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "och_x_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "\n",
    "vqbnn_op = create_bnn(OCC_CLASS_NO)\n",
    "vqbnn_op.load_weights(\"%s-%s\" % (BNN_PATH, OCC_FLAG))\n",
    "freeze(vqbnn_op, OCC_INPUT_DIM, POSTERIOR_NO)\n",
    "\n",
    "x_dims, y_dims = [OCC_INPUT_DIM], [OCC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "vqbnn = VQBNN(lambda x: vqbnn_op(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "evaluate_vqbnn(vqbnn, x_test, y_test, OCC_CLASS_NO, stat_period=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [EMG Data for Gestures Data Set](https://archive.ics.uci.edu/ml/datasets/EMG+data+for+gestures) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMG_FLAG = \"emg\"\n",
    "EMG_INPUT_DIM = 8\n",
    "EMG_CLASS_NO = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emg_data(dataset_no):\n",
    "    emg_path = DATASET_PATH + \"%s/\" % EMG_FLAG\n",
    "    channels, labels = [], []\n",
    "    dat_path = emg_path + \"%02d/\" % dataset_no\n",
    "    for file_name in os.listdir(dat_path):\n",
    "        with open(dat_path + file_name, 'r') as csvfile:\n",
    "            file_reader = csv.reader(csvfile, delimiter='\\t')\n",
    "            header = next(file_reader)\n",
    "            for row in file_reader:\n",
    "                if len(row) == 10:\n",
    "                    channels.append(tf.constant([float(v) for v in row[1:9]]))\n",
    "                    labels.append(tf.constant([int(row[9])]))\n",
    "    return channels, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regularizer = 10e4\n",
    "\n",
    "# x_train, y_train = tuple(zip(*[emg_data(i) for i in range(1, 33)]))\n",
    "x_train, y_train = tuple(zip(*[emg_data(i) for i in range(1, 10)]))\n",
    "x_train, y_train = tf.concat(x_train, 0), tf.concat(y_train, 0)\n",
    "x_train = x_train * regularizer\n",
    "# x_test, y_test = tuple(zip(*[emg_data(i) for i in range(33, 37)]))\n",
    "x_test, y_test = tuple(zip(*[emg_data(i) for i in range(36, 37)]))\n",
    "x_test, y_test = tf.concat(x_test, 0), tf.concat(y_test, 0)\n",
    "x_test = x_test * regularizer\n",
    "\n",
    "print(\"train dataset: %d\" % len(x_train))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())\n",
    "print(\"test dataset: %d\" % len(x_test))\n",
    "print(\"x sample: \", x_test[0].numpy())\n",
    "print(\"y sample: \", y_test[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(EMG_CLASS_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 30, 300\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "    time2 = time.time()\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = '({} sec) Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(time2 - time1,\n",
    "                              epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        bnn.save_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG), save_format='tf')\n",
    "        print(\"Snapshot saved.\")\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(EMG_CLASS_NO)\n",
    "bnn.load_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_nn(bnn, x_test, y_test, EMG_CLASS_NO, batch=100, stat_period=1, cutoff=0.9, print_stat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test VQ-BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "och_x_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "\n",
    "vqbnn_op = create_bnn(EMG_CLASS_NO)\n",
    "vqbnn_op.load_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG))\n",
    "freeze(vqbnn_op, EMG_INPUT_DIM, POSTERIOR_NO)\n",
    "\n",
    "x_dims, y_dims = [EMG_INPUT_DIM], [EMG_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "vqbnn = VQBNN(lambda x: vqbnn_op(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "evaluate_vqbnn(vqbnn, x_test, y_test, EMG_CLASS_NO, stat_period=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Localization Data for Person Activity Data Set](https://archive.ics.uci.edu/ml/datasets/Localization+Data+for+Person+Activity) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOC_FLAG = \"localization\"\n",
    "LOC_INPUT_DIM = 4\n",
    "LOC_CLASS_NO = 11\n",
    "TAG = [\"010-000-024-033\", \"010-000-030-096\", \"020-000-033-111\", \"020-000-032-221\"]\n",
    "ACTIVITY = [\"walking\",\"falling\",\"lying down\",\"lying\",\"sitting down\",\"sitting\",\"standing up from lying\",\"on all fours\",\"sitting on the ground\",\"standing up from sitting\",\"standing up from sitting on the ground\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_data():\n",
    "    file_path = DATASET_PATH + \"%s/%s\" % (LOC_FLAG, \"ConfLongDemo_JSI.txt\")\n",
    "    xs, ys = [], []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            xs.append(tf.constant([TAG.index(row[1])] + [float(v) for v in row[4:7]]))\n",
    "            ys.append(tf.constant([ACTIVITY.index(row[7])]))\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = localization_data()\n",
    "x_len, train_len = len(xs), int(len(xs) * 0.9)\n",
    "x_train, y_train, x_test, y_test = xs[:train_len], ys[:train_len], xs[train_len:], ys[train_len:]\n",
    "\n",
    "print(\"train dataset: %d, test datasets: %d\" % (len(x_train), len(x_test)))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 30, 3\n",
    "\n",
    "bnn = create_bnn(LOC_CLASS_NO)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "    time2 = time.time()\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = 'Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (BNN_PATH, LOC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(LOC_CLASS_NO)\n",
    "bnn.load_weights(\"%s-%s\" % (BNN_PATH, LOC_FLAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_nn(bnn, x_test, y_test, LOC_CLASS_NO, batch=100, stat_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test VQ-BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "och_x_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "\n",
    "vqbnn_op = create_bnn(LOC_CLASS_NO)\n",
    "vqbnn_op.load_weights(\"%s-%s\" % (BNN_PATH, LOC_FLAG))\n",
    "freeze(vqbnn_op, LOC_INPUT_DIM, POSTERIOR_NO)\n",
    "\n",
    "x_dims, y_dims = [LOC_INPUT_DIM], [LOC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "vqbnn = VQBNN(lambda x: vqbnn_op(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "evaluate_vqbnn(vqbnn, x_test, y_test, LOC_CLASS_NO, stat_period=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
