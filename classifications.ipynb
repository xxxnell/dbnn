{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow_probability import edward2 as ed\n",
    "from vqbnn.vqbnn import VQBNN\n",
    "from vqbnn.och import OCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig = False\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "\n",
    "if not savefig:\n",
    "    plt.rcParams[\"figure.figsize\"] = (4, 4)\n",
    "    plt.rcParams[\"font.size\"] = 15\n",
    "    plt.rcParams[\"figure.titlesize\"] = 25\n",
    "    plt.rcParams[\"axes.labelsize\"] = 20\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 15\n",
    "    plt.rcParams[\"legend.fontsize\"] = 13\n",
    "    plt.rcParams[\"lines.linewidth\"] = 2\n",
    "else:\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "    plt.rcParams[\"font.size\"] = 30\n",
    "    plt.rcParams[\"axes.labelsize\"] = 53\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 40\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 40\n",
    "    plt.rcParams[\"legend.fontsize\"] = 28\n",
    "    plt.rcParams[\"lines.linewidth\"] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants and hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "CLASSIFICATION_FLAG = \"classifications\"\n",
    "DATASET_PATH = 'datasets/'\n",
    "BNN_PATH = 'models_checkpoints/bnn'\n",
    "DAT_PATH = \"leaderboard/%s/\" % CLASSIFICATION_FLAG\n",
    "\n",
    "# experiments\n",
    "seed = 30\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "POSTERIOR_NO = 30\n",
    "CONFIDENCE_CUTOFF = 0.9\n",
    "\n",
    "# och parameters\n",
    "och_x1_params = {'k': 5, 'l': 5.0, 's': 1.0}\n",
    "och_x_params = {'k': 10, 'l': 0.01, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 0.01, 's': 1.0}\n",
    "\n",
    "# style\n",
    "alpha = 0.12\n",
    "colors = [\"tab:blue\", \"tab:green\", \"tab:purple\", \"tab:red\"]\n",
    "labels = [\"DNN\", \"MU\", \"DU\", \"VQ-BNN\"]\n",
    "guide_linestyle=(0, (1, 1))\n",
    "linestyles = [(0, (5, 1)), 'solid', (0, (5, 1)), 'solid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnn_categorical(model, xs):\n",
    "    logits = model(xs)\n",
    "    return tfd.Categorical(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, x_train, y_train, batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    indexes_batch = np.array_split(np.random.permutation(len(x_train)), len(x_train) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "\n",
    "    for indexes in indexes_batch:\n",
    "        xs = tf.stack([x_train[index] for index in indexes])\n",
    "        ys = tf.stack([y_train[index] for index in indexes])\n",
    "        train_step(model, xs, ys, optimizer, loss_ftn, *loss_metrics)\n",
    "    \n",
    "def train_step(model, x_batch, y_batch, optimizer, loss_ftn, *loss_metrics):\n",
    "    with tf.GradientTape() as tape:\n",
    "        losses = loss_ftn(model, x_batch, y_batch)\n",
    "    gradients = tape.gradient(losses[0], model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    for loss, loss_metric in zip(losses, loss_metrics):\n",
    "        loss_metric(loss)\n",
    "        \n",
    "def bnn_categorical_loss_ftn(model, xs, ys, length):\n",
    "    ys = tf.squeeze(ys, axis=-1)\n",
    "    nll = - tf.reduce_mean(bnn_categorical(model, xs).log_prob(ys))\n",
    "    kl = sum(model.losses) / length\n",
    "    loss = nll + kl\n",
    "    return loss, nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(model, x_test, y_test, depth, batch=1, stat_period=1, sample_no=POSTERIOR_NO, cutoff=CONFIDENCE_CUTOFF, print_stat=False):\n",
    "    cce_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "    mse_90_metric = tf.keras.metrics.Mean(name='mse-90')\n",
    "    cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "    cov_metric = tf.keras.metrics.Mean(name='cov')\n",
    "\n",
    "    indexes_batch = np.array_split(range(len(x_test)), len(x_test) / batch + 1)\n",
    "    indexes_batch = [indexes for indexes in indexes_batch if len(indexes) > 0]\n",
    "    for i, indexes in enumerate(indexes_batch):\n",
    "        start_time = time.time()\n",
    "        xs = tf.stack([x_test[index] for index in indexes])\n",
    "        ys = tf.stack([y_test[index] for index in indexes])\n",
    "        ys = tf.one_hot(tf.reshape(ys, [-1]), depth)\n",
    "        \n",
    "        prediction = [model(xs) for _ in range(sample_no)]\n",
    "        prediction = tf.nn.softmax(prediction)\n",
    "        prediction = tf.math.reduce_mean(prediction, axis=0)\n",
    "        filtered = [(p, y) for p, y in zip(prediction, ys) if tf.math.reduce_max(p) >= cutoff]\n",
    "        prediction_cutoff, ys_cutoff = tf.stack([p for p, _ in filtered]), tf.stack([y for _, y in filtered])\n",
    "        \n",
    "        if print_stat:\n",
    "            print(\"ys:\", np.array([y_test[index][0] for index in indexes]))\n",
    "            print(\"result: \", tf.math.argmax(prediction, axis=-1))\n",
    "            print(\"confidence: \", tf.math.reduce_max(prediction, axis=-1))\n",
    "        \n",
    "        mse = tf.keras.losses.MSE(ys, prediction)\n",
    "        cce = cce_object(ys, prediction)\n",
    "        mse_cutoff = tf.keras.losses.MSE(ys_cutoff, prediction_cutoff)\n",
    "\n",
    "        mse_metric(mse)\n",
    "        cce_metric(cce)\n",
    "        cov_metric(len(prediction_cutoff) / len(prediction))\n",
    "        if len(filtered) > 0:\n",
    "            mse_90_metric(mse_cutoff)\n",
    "    \n",
    "        if ((i + 1) % stat_period is 0) or ((i + 1) is len(indexes_batch)):\n",
    "            template = \"(%s, %.1f sec/epoch), Epoch %d/%d, MSE: %.4f, MSE-90: %.4f, CCE: %.4f, Cov-90: %.4f %%\"\n",
    "            print(template % (datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                              (time.time() - start_time) / stat_period,\n",
    "                              i + 1,\n",
    "                              len(indexes_batch),\n",
    "                              mse_metric.result(),\n",
    "                              mse_90_metric.result(),\n",
    "                              cce_metric.result(),\n",
    "                              cov_metric.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vqbnn(vqbnn, x_test, y_test, depth, stat_period=1, sample_no=POSTERIOR_NO, cutoff=CONFIDENCE_CUTOFF):\n",
    "    cce_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "    mse_metric = tf.keras.metrics.Mean(name='mse')\n",
    "    mse_90_metric = tf.keras.metrics.Mean(name='mse-90')\n",
    "    cce_metric = tf.keras.metrics.Mean(name='cce')\n",
    "    cov_metric = tf.keras.metrics.Mean(name='cov')\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(x_test, y_test)):\n",
    "        start_time = time.time()\n",
    "        ys = tf.one_hot(y, depth)\n",
    "        vqbnn.update(x)\n",
    "        \n",
    "        if sum([w for _, w in vqbnn.och_y.cws()]) > 0:\n",
    "            prediction = [tf.nn.softmax(c) * w for c, w in vqbnn.och_y.cws()]\n",
    "            prediction = tf.math.add_n(prediction)\n",
    "            filtered = [(p, y) for p, y in zip(prediction, ys) if tf.math.reduce_max(p) >= cutoff]\n",
    "            prediction_cutoff, ys_cutoff = tf.stack([p for p, _ in filtered]), tf.stack([y for _, y in filtered])\n",
    "\n",
    "            mse = tf.keras.losses.MSE(ys, prediction)\n",
    "            cce = cce_object(ys, prediction)\n",
    "            mse_cutoff = tf.keras.losses.MSE(ys_cutoff, prediction_cutoff)\n",
    "            \n",
    "            mse_metric(mse)\n",
    "            cov_metric(len(prediction_cutoff) / len(prediction))\n",
    "            cce_metric(cce)\n",
    "            if len(filtered) > 0:\n",
    "                mse_90_metric(mse_cutoff)\n",
    "    \n",
    "        if ((i + 1) % stat_period is 0) or ((i + 1) is len(x_test)):\n",
    "            template = \"(%s, %.1f sec/epoch), Epoch %d/%d, MSE: %.4f, MSE-90: %.4f, CCE: %.4f, Cov-90: %.4f\"\n",
    "            print(template % (datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                              (time.time() - start_time) / stat_period,\n",
    "                              i + 1,\n",
    "                              len(x_test),\n",
    "                              mse_metric.result(),\n",
    "                              mse_90_metric.result(),\n",
    "                              cce_metric.result(),\n",
    "                              cov_metric.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_NAMES = DENSE_1, DENSE_2, DENSE_3 = \"dense_1\", \"dense_2\", \"dense_3\"\n",
    "\n",
    "def create_dnn(classes_no, unit=50):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(unit, activation=tf.nn.relu, name=DENSE_1),\n",
    "        tf.keras.layers.Dense(unit, activation=tf.nn.relu, name=DENSE_2),\n",
    "        tf.keras.layers.Dense(unit, activation=tf.nn.relu, name=DENSE_3),\n",
    "        tf.keras.layers.Dense(classes_no, name=DENSE_3)\n",
    "    ])\n",
    "\n",
    "def create_bnn(classes_no, unit=50):\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(unit, activation=tf.nn.relu, name=DENSE_1),\n",
    "        tfp.layers.DenseFlipout(unit, activation=tf.nn.relu, name=DENSE_2),\n",
    "        tfp.layers.DenseFlipout(classes_no, name=DENSE_3)\n",
    "    ])\n",
    "\n",
    "def freeze(bnn, input_dim, posterior_no):\n",
    "    kernel_d1, bias_d1 = [], []\n",
    "    kernel_d2, bias_d2 = [], []\n",
    "    kernel_d3, bias_d3 = [], []\n",
    "    \n",
    "    layer1 = bnn.get_layer(DENSE_1) \n",
    "    layer2 = bnn.get_layer(DENSE_2) \n",
    "    layer3 = bnn.get_layer(DENSE_3) \n",
    "\n",
    "    layer1.kernel_posterior_tensor_fn = lambda d: append(kernel_d1, d.sample())\n",
    "    layer1.bias_posterior_tensor_fn = lambda d: append(bias_d1, d.sample())\n",
    "    layer2.kernel_posterior_tensor_fn = lambda d: append(kernel_d2, d.sample())\n",
    "    layer2.bias_posterior_tensor_fn = lambda d: append(bias_d2, d.sample())\n",
    "    layer3.kernel_posterior_tensor_fn = lambda d: append(kernel_d3, d.sample())\n",
    "    layer3.bias_posterior_tensor_fn = lambda d: append(bias_d3, d.sample())\n",
    "        \n",
    "    _ = [bnn(tf.random.normal((1, input_dim))) for _ in range(posterior_no)]\n",
    "    \n",
    "    layer1.kernel_posterior_tensor_fn = lambda d: kernel_d1[random.randint(0, posterior_no - 1)]\n",
    "    layer1.bias_posterior_tensor_fn = lambda d: bias_d1[random.randint(0, posterior_no - 1)]\n",
    "    layer2.kernel_posterior_tensor_fn = lambda d: kernel_d2[random.randint(0, posterior_no - 1)]\n",
    "    layer2.bias_posterior_tensor_fn = lambda d: bias_d2[random.randint(0, posterior_no - 1)]\n",
    "    layer3.kernel_posterior_tensor_fn = lambda d: kernel_d3[random.randint(0, posterior_no - 1)]\n",
    "    layer3.bias_posterior_tensor_fn = lambda d: bias_d3[random.randint(0, posterior_no - 1)]\n",
    "    \n",
    "def append(xs, x):\n",
    "    xs.append(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "* [Occupancy Detection Data Set (Classification)](#Occupancy-Detection-Data-Set-(Classification))\n",
    "* [EMG Data for Gestures Data Set (Classification)](#EMG-Data-for-Gestures-Data-Set-(Classification))\n",
    "* [Localization Data for Person Activity Data Set (Classification)](#Localization-Data-for-Person-Activity-Data-Set-(Classification))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Occupancy Detection Data Set](https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCC_FLAG = \"occupancy\"\n",
    "OCC_INPUT_DIM = 5\n",
    "OCC_CLASS_NO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_file_names, test_file_names = [\"datatraining.txt\"], [\"datatest.txt\", \"datatest2.txt\"]\n",
    "\n",
    "def occupancy_data(file_name):\n",
    "    occupancy_path = DATASET_PATH + \"%s/\" % OCC_FLAG\n",
    "    x, y = [], []\n",
    "    with open(occupancy_path + file_name, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            x.append(tf.constant([float(v) for v in row[2: 7]]))\n",
    "            y.append(tf.constant([int(row[7])]))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 8143, Testset size: 12417\n",
      "x sample:  [2.3180000e+01 2.7271999e+01 4.2600000e+02 7.2125000e+02 4.7929883e-03]\n",
      "y sample:  [1]\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = occupancy_data(train_file_names[0])\n",
    "x_test, y_test = tuple(zip(*[occupancy_data(test_file_name) for test_file_name in test_file_names]))\n",
    "x_test, y_test = tf.concat(x_test, 0), tf.concat(y_test, 0)\n",
    "\n",
    "print(\"Train dataset: %d, Testset size: %d\" % (len(x_train), len(x_test)))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(OCC_CLASS_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, batch = 50, 100\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "    time2 = time.time()\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = '(%.2f sec) Epoch: %d, Loss: %.4f, NLL: %.4f'\n",
    "        print(template % (time2 - time1,\n",
    "                          epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (BNN_PATH, OCC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x12ee9bad0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnn = create_bnn(OCC_CLASS_NO)\n",
    "bnn.load_weights(\"%s-%s\" % (BNN_PATH, OCC_FLAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0107 14:43:24.806595 4552199616 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow_probability/python/layers/util.py:104: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020-01-07 14:43:25, 0.6 sec/epoch), Epoch 1/10, MSE: 0.0048, MSE-90: 0.0048, CCE: 0.0718, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:26, 0.7 sec/epoch), Epoch 2/10, MSE: 0.0031, MSE-90: 0.0031, CCE: 0.0546, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:26, 0.7 sec/epoch), Epoch 3/10, MSE: 0.0035, MSE-90: 0.0035, CCE: 0.0591, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:27, 0.7 sec/epoch), Epoch 4/10, MSE: 0.0035, MSE-90: 0.0035, CCE: 0.0596, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:28, 0.6 sec/epoch), Epoch 5/10, MSE: 0.0036, MSE-90: 0.0036, CCE: 0.0609, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:28, 0.6 sec/epoch), Epoch 6/10, MSE: 0.0037, MSE-90: 0.0037, CCE: 0.0621, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:29, 0.8 sec/epoch), Epoch 7/10, MSE: 0.0039, MSE-90: 0.0039, CCE: 0.0639, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:30, 0.9 sec/epoch), Epoch 8/10, MSE: 0.0037, MSE-90: 0.0037, CCE: 0.0621, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:31, 0.8 sec/epoch), Epoch 9/10, MSE: 0.0038, MSE-90: 0.0038, CCE: 0.0631, Cov-90: 100.0000 %\n",
      "(2020-01-07 14:43:31, 0.6 sec/epoch), Epoch 10/10, MSE: 0.0036, MSE-90: 0.0036, CCE: 0.0609, Cov-90: 100.0000 %\n"
     ]
    }
   ],
   "source": [
    "evaluate_nn(bnn, x_test, y_test, OCC_CLASS_NO, batch=1, stat_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test VQ-BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020-01-07 14:43:43, 0.0 sec/epoch), Epoch 10/12417, MSE: 0.0013, MSE-90: 0.0013, CCE: 0.0351, Cov-90: 100.0000\n",
      "(2020-01-07 14:43:43, 0.0 sec/epoch), Epoch 20/12417, MSE: 0.0009, MSE-90: 0.0009, CCE: 0.0292, Cov-90: 100.0000\n",
      "(2020-01-07 14:43:43, 0.0 sec/epoch), Epoch 30/12417, MSE: 0.0238, MSE-90: 0.0012, CCE: 0.1046, Cov-90: 79.3103\n",
      "(2020-01-07 14:43:43, 0.0 sec/epoch), Epoch 40/12417, MSE: 0.0178, MSE-90: 0.0009, CCE: 0.0812, Cov-90: 84.6154\n",
      "(2020-01-07 14:43:43, 0.0 sec/epoch), Epoch 50/12417, MSE: 0.0141, MSE-90: 0.0007, CCE: 0.0662, Cov-90: 87.7551\n",
      "(2020-01-07 14:43:44, 0.0 sec/epoch), Epoch 60/12417, MSE: 0.0118, MSE-90: 0.0006, CCE: 0.0561, Cov-90: 89.8305\n",
      "(2020-01-07 14:43:44, 0.0 sec/epoch), Epoch 70/12417, MSE: 0.0104, MSE-90: 0.0005, CCE: 0.0500, Cov-90: 91.0448\n",
      "(2020-01-07 14:43:44, 0.0 sec/epoch), Epoch 80/12417, MSE: 0.0128, MSE-90: 0.0004, CCE: 0.0543, Cov-90: 90.9091\n",
      "(2020-01-07 14:43:44, 0.0 sec/epoch), Epoch 90/12417, MSE: 0.0197, MSE-90: 0.0005, CCE: 0.0726, Cov-90: 88.5057\n",
      "(2020-01-07 14:43:44, 0.0 sec/epoch), Epoch 100/12417, MSE: 0.0283, MSE-90: 0.0121, CCE: 0.1340, Cov-90: 89.5833\n",
      "(2020-01-07 14:43:45, 0.0 sec/epoch), Epoch 110/12417, MSE: 0.0256, MSE-90: 0.0108, CCE: 0.1223, Cov-90: 90.5660\n",
      "(2020-01-07 14:43:45, 0.0 sec/epoch), Epoch 120/12417, MSE: 0.0257, MSE-90: 0.0099, CCE: 0.1186, Cov-90: 90.5172\n",
      "(2020-01-07 14:43:45, 0.0 sec/epoch), Epoch 130/12417, MSE: 0.0247, MSE-90: 0.0092, CCE: 0.1146, Cov-90: 89.6825\n",
      "(2020-01-07 14:43:45, 0.0 sec/epoch), Epoch 140/12417, MSE: 0.0229, MSE-90: 0.0085, CCE: 0.1075, Cov-90: 90.4412\n",
      "(2020-01-07 14:43:46, 0.0 sec/epoch), Epoch 150/12417, MSE: 0.0214, MSE-90: 0.0080, CCE: 0.1027, Cov-90: 91.0959\n",
      "(2020-01-07 14:43:46, 0.0 sec/epoch), Epoch 160/12417, MSE: 0.0201, MSE-90: 0.0075, CCE: 0.0986, Cov-90: 91.6667\n",
      "(2020-01-07 14:43:46, 0.0 sec/epoch), Epoch 170/12417, MSE: 0.0192, MSE-90: 0.0073, CCE: 0.0965, Cov-90: 91.5663\n",
      "(2020-01-07 14:43:46, 0.0 sec/epoch), Epoch 180/12417, MSE: 0.0184, MSE-90: 0.0072, CCE: 0.0954, Cov-90: 92.0455\n",
      "(2020-01-07 14:43:46, 0.0 sec/epoch), Epoch 190/12417, MSE: 0.0198, MSE-90: 0.0070, CCE: 0.0988, Cov-90: 91.9355\n",
      "(2020-01-07 14:43:47, 0.0 sec/epoch), Epoch 200/12417, MSE: 0.0238, MSE-90: 0.0070, CCE: 0.1113, Cov-90: 87.7551\n",
      "(2020-01-07 14:43:47, 0.0 sec/epoch), Epoch 210/12417, MSE: 0.0619, MSE-90: 0.0493, CCE: 0.2247, Cov-90: 87.8641\n",
      "(2020-01-07 14:43:47, 0.0 sec/epoch), Epoch 220/12417, MSE: 0.0898, MSE-90: 0.0708, CCE: 0.3052, Cov-90: 86.1111\n",
      "(2020-01-07 14:43:48, 0.0 sec/epoch), Epoch 230/12417, MSE: 0.0866, MSE-90: 0.0687, CCE: 0.2964, Cov-90: 84.9557\n",
      "(2020-01-07 14:43:48, 0.0 sec/epoch), Epoch 240/12417, MSE: 0.0829, MSE-90: 0.0653, CCE: 0.2838, Cov-90: 85.5932\n",
      "(2020-01-07 14:43:48, 0.0 sec/epoch), Epoch 250/12417, MSE: 0.0795, MSE-90: 0.0622, CCE: 0.2723, Cov-90: 86.1789\n",
      "(2020-01-07 14:43:48, 0.0 sec/epoch), Epoch 260/12417, MSE: 0.0764, MSE-90: 0.0594, CCE: 0.2617, Cov-90: 86.7188\n",
      "(2020-01-07 14:43:49, 0.0 sec/epoch), Epoch 270/12417, MSE: 0.0736, MSE-90: 0.0569, CCE: 0.2518, Cov-90: 87.2180\n",
      "(2020-01-07 14:43:49, 0.0 sec/epoch), Epoch 280/12417, MSE: 0.0709, MSE-90: 0.0545, CCE: 0.2427, Cov-90: 87.6812\n",
      "(2020-01-07 14:43:49, 0.0 sec/epoch), Epoch 290/12417, MSE: 0.0684, MSE-90: 0.0523, CCE: 0.2342, Cov-90: 88.1119\n",
      "(2020-01-07 14:43:49, 0.0 sec/epoch), Epoch 300/12417, MSE: 0.0661, MSE-90: 0.0503, CCE: 0.2263, Cov-90: 88.5135\n",
      "(2020-01-07 14:43:49, 0.0 sec/epoch), Epoch 310/12417, MSE: 0.0639, MSE-90: 0.0485, CCE: 0.2189, Cov-90: 88.8889\n",
      "(2020-01-07 14:43:50, 0.0 sec/epoch), Epoch 320/12417, MSE: 0.0619, MSE-90: 0.0468, CCE: 0.2120, Cov-90: 89.2405\n",
      "(2020-01-07 14:43:50, 0.0 sec/epoch), Epoch 330/12417, MSE: 0.0600, MSE-90: 0.0452, CCE: 0.2055, Cov-90: 89.5705\n",
      "(2020-01-07 14:43:51, 0.0 sec/epoch), Epoch 340/12417, MSE: 0.0582, MSE-90: 0.0437, CCE: 0.1994, Cov-90: 89.8810\n",
      "(2020-01-07 14:43:51, 0.0 sec/epoch), Epoch 350/12417, MSE: 0.0565, MSE-90: 0.0423, CCE: 0.1936, Cov-90: 90.1734\n",
      "(2020-01-07 14:43:51, 0.0 sec/epoch), Epoch 360/12417, MSE: 0.0550, MSE-90: 0.0410, CCE: 0.1882, Cov-90: 90.4494\n",
      "(2020-01-07 14:43:52, 0.0 sec/epoch), Epoch 370/12417, MSE: 0.0535, MSE-90: 0.0397, CCE: 0.1830, Cov-90: 90.7104\n",
      "(2020-01-07 14:43:52, 0.0 sec/epoch), Epoch 380/12417, MSE: 0.0520, MSE-90: 0.0386, CCE: 0.1782, Cov-90: 90.9574\n",
      "(2020-01-07 14:43:53, 0.0 sec/epoch), Epoch 390/12417, MSE: 0.0507, MSE-90: 0.0375, CCE: 0.1735, Cov-90: 91.1917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e7aaf03085ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvqbnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVQBNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvqbnn_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moch_x_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mevaluate_vqbnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvqbnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOCC_CLASS_NO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstat_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-19ffa06adc8b>\u001b[0m in \u001b[0;36mevaluate_vqbnn\u001b[0;34m(vqbnn, x_test, y_test, depth, stat_period, sample_no, cutoff)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvqbnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvqbnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-19ffa06adc8b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvqbnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvqbnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    910\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m   6357\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6358\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_eager_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6359\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6361\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py\u001b[0m in \u001b[0;36mscope_name\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    672\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;34m\"\"\"Sets scope name for the current thread.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "och_x_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "\n",
    "vqbnn_op = create_bnn(OCC_CLASS_NO)\n",
    "vqbnn_op.load_weights(\"%s-%s\" % (BNN_PATH, OCC_FLAG))\n",
    "freeze(vqbnn_op, OCC_INPUT_DIM, POSTERIOR_NO)\n",
    "\n",
    "x_dims, y_dims = [OCC_INPUT_DIM], [OCC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "vqbnn = VQBNN(lambda x: vqbnn_op(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "evaluate_vqbnn(vqbnn, x_test, y_test, OCC_CLASS_NO, stat_period=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [EMG Data for Gestures Data Set](https://archive.ics.uci.edu/ml/datasets/EMG+data+for+gestures) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMG_FLAG = \"emg\"\n",
    "EMG_INPUT_DIM = 8\n",
    "EMG_CLASS_NO = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emg_data(dataset_no):\n",
    "    emg_path = DATASET_PATH + \"%s/\" % EMG_FLAG\n",
    "    channels, labels = [], []\n",
    "    dat_path = emg_path + \"%02d/\" % dataset_no\n",
    "    for file_name in os.listdir(dat_path):\n",
    "        with open(dat_path + file_name, 'r') as csvfile:\n",
    "            file_reader = csv.reader(csvfile, delimiter='\\t')\n",
    "            header = next(file_reader)\n",
    "            for row in file_reader:\n",
    "                if len(row) == 10:\n",
    "                    channels.append(tf.constant([float(v) for v in row[1:9]]))\n",
    "                    labels.append(tf.constant([int(row[9])]))\n",
    "    return channels, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 1062681\n",
      "x sample:  [ 1. -2. -1. -3.  0. -1.  0. -1.]\n",
      "y sample:  [0]\n",
      "test dataset: 101754\n",
      "x sample:  [-1.  0. -4. -4.  2.  1. -1. -1.]\n",
      "y sample:  [0]\n"
     ]
    }
   ],
   "source": [
    "regularizer = 10e4\n",
    "\n",
    "# x_train, y_train = tuple(zip(*[emg_data(i) for i in range(1, 33)]))\n",
    "x_train, y_train = tuple(zip(*[emg_data(i) for i in range(1, 10)]))\n",
    "x_train, y_train = tf.concat(x_train, 0), tf.concat(y_train, 0)\n",
    "x_train = x_train * regularizer\n",
    "# x_test, y_test = tuple(zip(*[emg_data(i) for i in range(33, 37)]))\n",
    "x_test, y_test = tuple(zip(*[emg_data(i) for i in range(36, 37)]))\n",
    "x_test, y_test = tf.concat(x_test, 0), tf.concat(y_test, 0)\n",
    "x_test = x_test * regularizer\n",
    "\n",
    "print(\"train dataset: %d\" % len(x_train))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())\n",
    "print(\"test dataset: %d\" % len(x_test))\n",
    "print(\"x sample: \", x_test[0].numpy())\n",
    "print(\"y sample: \", y_test[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(EMG_CLASS_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 30, 300\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "    time2 = time.time()\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = '({} sec) Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(time2 - time1,\n",
    "                              epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        bnn.save_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG), save_format='tf')\n",
    "        print(\"Snapshot saved.\")\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(EMG_CLASS_NO)\n",
    "bnn.load_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_nn(bnn, x_test, y_test, EMG_CLASS_NO, batch=100, stat_period=1, cutoff=0.9, print_stat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test VQ-BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "och_x_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "\n",
    "vqbnn_op = create_bnn(EMG_CLASS_NO)\n",
    "vqbnn_op.load_weights(\"%s-%s\" % (BNN_PATH, EMG_FLAG))\n",
    "freeze(vqbnn_op, EMG_INPUT_DIM, POSTERIOR_NO)\n",
    "\n",
    "x_dims, y_dims = [EMG_INPUT_DIM], [EMG_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "vqbnn = VQBNN(lambda x: vqbnn_op(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "evaluate_vqbnn(vqbnn, x_test, y_test, EMG_CLASS_NO, stat_period=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Localization Data for Person Activity Data Set](https://archive.ics.uci.edu/ml/datasets/Localization+Data+for+Person+Activity) (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOC_FLAG = \"localization\"\n",
    "LOC_INPUT_DIM = 4\n",
    "LOC_CLASS_NO = 11\n",
    "TAG = [\"010-000-024-033\", \"010-000-030-096\", \"020-000-033-111\", \"020-000-032-221\"]\n",
    "ACTIVITY = [\"walking\",\"falling\",\"lying down\",\"lying\",\"sitting down\",\"sitting\",\"standing up from lying\",\"on all fours\",\"sitting on the ground\",\"standing up from sitting\",\"standing up from sitting on the ground\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_data():\n",
    "    file_path = DATASET_PATH + \"%s/%s\" % (LOC_FLAG, \"ConfLongDemo_JSI.txt\")\n",
    "    xs, ys = [], []\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        file_reader = csv.reader(csvfile, delimiter=',')\n",
    "        header = next(file_reader)\n",
    "        for row in file_reader:\n",
    "            xs.append(tf.constant([TAG.index(row[1])] + [float(v) for v in row[4:7]]))\n",
    "            ys.append(tf.constant([ACTIVITY.index(row[7])]))\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = localization_data()\n",
    "x_len, train_len = len(xs), int(len(xs) * 0.9)\n",
    "x_train, y_train, x_test, y_test = xs[:train_len], ys[:train_len], xs[train_len:], ys[train_len:]\n",
    "\n",
    "print(\"train dataset: %d, test datasets: %d\" % (len(x_train), len(x_test)))\n",
    "print(\"x sample: \", x_train[0].numpy())\n",
    "print(\"y sample: \", y_train[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs, batch = 30, 3\n",
    "\n",
    "bnn = create_bnn(LOC_CLASS_NO)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_ftn = lambda model, xs, ys: bnn_categorical_loss_ftn(model, xs, ys, len(x_train))\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "nll_loss = tf.keras.metrics.Mean(name='nll_loss')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    train_epoch(bnn, x_train, y_train, batch, optimizer, loss_ftn, train_loss, nll_loss)\n",
    "    time2 = time.time()\n",
    "        \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        template = 'Epoch {}, Loss: {}, NLL: {}'\n",
    "        print(template.format(epoch + 1,\n",
    "                              train_loss.result(),\n",
    "                              nll_loss.result()))\n",
    "        train_loss.reset_states()\n",
    "        nll_loss.reset_states()\n",
    "        \n",
    "bnn.save_weights(\"%s-%s\" % (BNN_PATH, LOC_FLAG), save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = create_bnn(LOC_CLASS_NO)\n",
    "bnn.load_weights(\"%s-%s\" % (BNN_PATH, LOC_FLAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_nn(bnn, x_test, y_test, LOC_CLASS_NO, batch=100, stat_period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test VQ-BNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "och_x_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "och_y_params = {'k': 10, 'l': 1.3, 's': 1.0}\n",
    "\n",
    "vqbnn_op = create_bnn(LOC_CLASS_NO)\n",
    "vqbnn_op.load_weights(\"%s-%s\" % (BNN_PATH, LOC_FLAG))\n",
    "freeze(vqbnn_op, LOC_INPUT_DIM, POSTERIOR_NO)\n",
    "\n",
    "x_dims, y_dims = [LOC_INPUT_DIM], [LOC_CLASS_NO]\n",
    "och_x = OCH(**och_x_params, dims=x_dims, hash_no=1)\n",
    "och_y = OCH(**och_y_params, dims=y_dims, hash_no=1)\n",
    "och_x_1 = OCH(**och_x1_params, dims=x_dims[1:], hash_no=3, cs=[])\n",
    "vqbnn = VQBNN(lambda x: vqbnn_op(tf.stack([x[0]])), och_x_1, och_x, och_y)\n",
    "\n",
    "evaluate_vqbnn(vqbnn, x_test, y_test, LOC_CLASS_NO, stat_period=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
